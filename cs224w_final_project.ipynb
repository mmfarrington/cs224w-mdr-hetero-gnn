{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0-aD2yojKk"
      },
      "source": [
        "# CS224W Final Project Notebook\n",
        "#### Miriam Farrington\n",
        "#### December 2025\n",
        "\n",
        "Description: This Colab notebook contains the code for the project **Multimodal Heterogeneous Graph Neural Networks for Predicting Adverse Device Events**.\n",
        "\n",
        "Summary:\n",
        "\n",
        "> This project applies heterogeneous graph neural networks (HGNNs) to post‑market surveillance of FDA‑approved medical devices by modeling adverse event relationships using openFDA Medical Device Reporting (MDR) data. The notebook constructs a heterogeneous graph over the 2024 MDR device report snapshot (January 2024), with nodes for reports, devices, manufacturers, and events, and edges such as report–mentions–device and manufacturer–makes–device. The main task is supervised link prediction on the report–mentions–device relation using compact device features and ID‑based embeddings, comparing several heterogeneous GNN architectures (including HGT and R‑GCN) under random and temporally constrained splits, with custom negative sampling and evaluation via AUROC and F1 on held‑out report–device edges.\n",
        "\n",
        "#### Datasets Used\n",
        "\n",
        "* MDR Data: https://www.fda.gov/medical-devices/medical-device-reporting-mdr-how-report-medical-device-problems/mdr-data-files#download\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ty3nd8Sj63m"
      },
      "source": [
        "# 1) Colab Setup and Package Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUplz1OvPUnt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lc_H8uvobed",
        "outputId": "944c1b93-ece9-44a5-dfe2-798bfb8b4f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9infHCSmfKhO"
      },
      "outputs": [],
      "source": [
        "# Clean conflicting installs first\n",
        "!pip -q uninstall -y torch torchvision torchaudio torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric pyg-lib 2>/dev/null || true\n",
        "\n",
        "# Install CUDA 12.1 build\n",
        "!pip -q install --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "\n",
        "# Install PyG wheels\n",
        "!pip -q install -f https://data.pyg.org/whl/torch-2.3.1+cu121.html \\\n",
        "  torch-scatter==2.1.2 torch-sparse==0.6.18 pyg-lib torch-geometric==2.6.1\n",
        "\n",
        "# dependencies\n",
        "!pip -q install torchmetrics==1.4.0.post0 pytorch-lightning==2.4.0 pandas pyarrow fastparquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gbIpa-9DfWF2"
      },
      "outputs": [],
      "source": [
        "import os, pandas as pd, numpy as np, torch, torch_geometric, torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.transforms import ToUndirected, RandomLinkSplit\n",
        "from torch_geometric.nn import RGCNConv, to_hetero\n",
        "from torchmetrics.classification import BinaryAUROC, BinaryF1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eWfmD53s12q",
        "outputId": "612f15c0-3c9b-4b06-d553-611bc32695a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/Stanford/device2024.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/drive/My Drive/Stanford/device2024.zip or\n",
            "        /content/drive/My Drive/Stanford/device2024.zip.zip, and cannot find /content/drive/My Drive/Stanford/device2024.zip.ZIP, period.\n",
            "Archive:  /content/drive/My Drive/Stanford/foitext2024.zip\n",
            "  inflating: /content/drive/My Drive/Stanford/foitext2024.txt  \n"
          ]
        }
      ],
      "source": [
        "#unzip data\n",
        "!unzip -o \"/content/drive/My Drive/Stanford/device2024.zip\" -d \"/content/drive/My Drive/Stanford/\"\n",
        "!unzip -o \"/content/drive/My Drive/Stanford/foitext2024.zip\" -d \"/content/drive/My Drive/Stanford/\"\n",
        "# drive paths\n",
        "root_dir   = \"/content/drive/My Drive/Stanford\"\n",
        "device_fp  = os.path.join(root_dir, \"DEVICE2024.txt\")\n",
        "foitext_fp = os.path.join(root_dir, \"FOITEXT2024.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zFqaOTK2mMrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eae5d26-449d-49ab-d4f4-789b981864da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121 12.1\n",
            "2.6.1\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# validations\n",
        "print(torch.__version__, torch.version.cuda)\n",
        "print(torch_geometric.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeo5NKtkDgv"
      },
      "source": [
        "##2) load + clean + create canonical keys\n",
        "### starting with a minimal subgraph first\n",
        "### schema:\n",
        "* nodes:\n",
        "  * manufacturer (MANUFACTURER_D_NAME)\n",
        "  * device (combo: MANUFACTURER_D_NAME || MODEL_NUMBER || CATALOG_NUMBER || UDI-DI)\n",
        "  * report (MDR_REPORT_KEY)\n",
        "  * event (from DEVICE_EVENT_KEY)\n",
        "\n",
        "* edges:\n",
        "  * (manufacturer) — makes -> (device)\n",
        "  * (report) — mentions -> (device)\n",
        "  * (event) — involves -> (device) (created if DEVICE_EVENT_KEY exists)\n",
        "\n",
        "* features:\n",
        "  * device categorical indices: BRAND_NAME, GENERIC_NAME, DEVICE_REPORT_PRODUCT_CODE\n",
        "  * device flags: IMPLANT_FLAG, COMBINATION_PRODUCT_FLAG (small embeddings)\n",
        "  * manufacturer: ID embedding only\n",
        "  * report/event: ID embedding only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "hy5lZsJcjWMr",
        "outputId": "1631b558-4de9-4556-b55c-6e740124ef27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1835814939.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  device_df = device_df.applymap(lambda x: x.strip() if isinstance(x, str) else x).fillna(\"\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          device_key      MANUFACTURER_D_NAME  \\\n",
              "0  AIZU OLYMPUS CO., LTD.||CF-H180AL||||049531702...   AIZU OLYMPUS CO., LTD.   \n",
              "1  AIZU OLYMPUS CO., LTD.||BF-XP190||||0495317034...   AIZU OLYMPUS CO., LTD.   \n",
              "2                AIZU OLYMPUS CO., LTD.||BF-Q290||||   AIZU OLYMPUS CO., LTD.   \n",
              "3  INTUITIVE SURGICAL, INC||470205-17||470205||00...  INTUITIVE SURGICAL, INC   \n",
              "4  INTUITIVE SURGICAL, INC||490206-01||490206||00...  INTUITIVE SURGICAL, INC   \n",
              "\n",
              "  MODEL_NUMBER CATALOG_NUMBER                           BRAND_NAME  \\\n",
              "0    CF-H180AL                      EVIS EXERA II COLONOVIDEOSCOPE   \n",
              "1     BF-XP190                    EVIS EXERA III BRONCHOVIDEOSCOPE   \n",
              "2      BF-Q290                 EVIS LUCERA ELITE BRONCHOVIDEOSCOPE   \n",
              "3    470205-17         470205                            ENDOWRIST   \n",
              "4    490206-01         490206                                  ION   \n",
              "\n",
              "                  GENERIC_NAME DEVICE_REPORT_PRODUCT_CODE IMPLANT_FLAG  \\\n",
              "0             COLONOVIDEOSCOPE                        FDF                \n",
              "1            BRONCHOVIDEOSCOPE                        EOQ                \n",
              "2            BRONCHOVIDEOSCOPE                        EOQ                \n",
              "3  FENESTRATED BIPOLAR FORCEPS                        NAY                \n",
              "4                 VISION PROBE                        EOQ                \n",
              "\n",
              "  COMBINATION_PRODUCT_FLAG          UDI-DI  device_id  \\\n",
              "0                        N  04953170202339          0   \n",
              "1                        N  04953170342134          1   \n",
              "2                        N                          2   \n",
              "3                        N  00886874112359          3   \n",
              "4                        N  00886874121184          4   \n",
              "\n",
              "                                      text_for_embed  \n",
              "0  EVIS EXERA II COLONOVIDEOSCOPE ; COLONOVIDEOSC...  \n",
              "1  EVIS EXERA III BRONCHOVIDEOSCOPE ; BRONCHOVIDE...  \n",
              "2  EVIS LUCERA ELITE BRONCHOVIDEOSCOPE ; BRONCHOV...  \n",
              "3  ENDOWRIST ; FENESTRATED BIPOLAR FORCEPS ; prod...  \n",
              "4              ION ; VISION PROBE ; product code EOQ  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93010d3e-2b18-4143-82fd-7172be598dc9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>device_key</th>\n",
              "      <th>MANUFACTURER_D_NAME</th>\n",
              "      <th>MODEL_NUMBER</th>\n",
              "      <th>CATALOG_NUMBER</th>\n",
              "      <th>BRAND_NAME</th>\n",
              "      <th>GENERIC_NAME</th>\n",
              "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
              "      <th>IMPLANT_FLAG</th>\n",
              "      <th>COMBINATION_PRODUCT_FLAG</th>\n",
              "      <th>UDI-DI</th>\n",
              "      <th>device_id</th>\n",
              "      <th>text_for_embed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AIZU OLYMPUS CO., LTD.||CF-H180AL||||049531702...</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.</td>\n",
              "      <td>CF-H180AL</td>\n",
              "      <td></td>\n",
              "      <td>EVIS EXERA II COLONOVIDEOSCOPE</td>\n",
              "      <td>COLONOVIDEOSCOPE</td>\n",
              "      <td>FDF</td>\n",
              "      <td></td>\n",
              "      <td>N</td>\n",
              "      <td>04953170202339</td>\n",
              "      <td>0</td>\n",
              "      <td>EVIS EXERA II COLONOVIDEOSCOPE ; COLONOVIDEOSC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AIZU OLYMPUS CO., LTD.||BF-XP190||||0495317034...</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.</td>\n",
              "      <td>BF-XP190</td>\n",
              "      <td></td>\n",
              "      <td>EVIS EXERA III BRONCHOVIDEOSCOPE</td>\n",
              "      <td>BRONCHOVIDEOSCOPE</td>\n",
              "      <td>EOQ</td>\n",
              "      <td></td>\n",
              "      <td>N</td>\n",
              "      <td>04953170342134</td>\n",
              "      <td>1</td>\n",
              "      <td>EVIS EXERA III BRONCHOVIDEOSCOPE ; BRONCHOVIDE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AIZU OLYMPUS CO., LTD.||BF-Q290||||</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.</td>\n",
              "      <td>BF-Q290</td>\n",
              "      <td></td>\n",
              "      <td>EVIS LUCERA ELITE BRONCHOVIDEOSCOPE</td>\n",
              "      <td>BRONCHOVIDEOSCOPE</td>\n",
              "      <td>EOQ</td>\n",
              "      <td></td>\n",
              "      <td>N</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>EVIS LUCERA ELITE BRONCHOVIDEOSCOPE ; BRONCHOV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTUITIVE SURGICAL, INC||470205-17||470205||00...</td>\n",
              "      <td>INTUITIVE SURGICAL, INC</td>\n",
              "      <td>470205-17</td>\n",
              "      <td>470205</td>\n",
              "      <td>ENDOWRIST</td>\n",
              "      <td>FENESTRATED BIPOLAR FORCEPS</td>\n",
              "      <td>NAY</td>\n",
              "      <td></td>\n",
              "      <td>N</td>\n",
              "      <td>00886874112359</td>\n",
              "      <td>3</td>\n",
              "      <td>ENDOWRIST ; FENESTRATED BIPOLAR FORCEPS ; prod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>INTUITIVE SURGICAL, INC||490206-01||490206||00...</td>\n",
              "      <td>INTUITIVE SURGICAL, INC</td>\n",
              "      <td>490206-01</td>\n",
              "      <td>490206</td>\n",
              "      <td>ION</td>\n",
              "      <td>VISION PROBE</td>\n",
              "      <td>EOQ</td>\n",
              "      <td></td>\n",
              "      <td>N</td>\n",
              "      <td>00886874121184</td>\n",
              "      <td>4</td>\n",
              "      <td>ION ; VISION PROBE ; product code EOQ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93010d3e-2b18-4143-82fd-7172be598dc9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-93010d3e-2b18-4143-82fd-7172be598dc9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-93010d3e-2b18-4143-82fd-7172be598dc9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-be7d0ce9-2f48-4478-86a3-86b9d9af22c8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be7d0ce9-2f48-4478-86a3-86b9d9af22c8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-be7d0ce9-2f48-4478-86a3-86b9d9af22c8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "device_keys",
              "summary": "{\n  \"name\": \"device_keys\",\n  \"rows\": 13860,\n  \"fields\": [\n    {\n      \"column\": \"device_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11635,\n        \"samples\": [\n          \"TOSOH HI-TEC, INC.||AIA-2000 ST||022100||04560189284616\",\n          \"ALTATEC GMBH||K1042.3816||||\",\n          \"BOSTON SCIENTIFIC NEUROMODULATION||DB-4600C||DB-4600C||08714729820802\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MANUFACTURER_D_NAME\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 899,\n        \"samples\": [\n          \"SHENZHEN MINDRAY BIO-MEDICAL ELECTRONICS CO., LTD\",\n          \"NEURO - VILLALBA\",\n          \"GE HEALTHCARE AUSTRIA GMBH & CO OG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MODEL_NUMBER\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7955,\n        \"samples\": [\n          \"10621-355MP\",\n          \"72404232\",\n          \"MMT-1881\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CATALOG_NUMBER\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5736,\n        \"samples\": [\n          \"DDMB1D4\",\n          \"10675\",\n          \"BTA5215\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BRAND_NAME\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5043,\n        \"samples\": [\n          \"PROGRASAP FORCEP FOR DAVINCI ROBOT\",\n          \"INSPIRIS RESILIA AORTIC VALVE\",\n          \"INJ. OPTIV DH, W/OEM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GENERIC_NAME\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1997,\n        \"samples\": [\n          \"ULTRASONIC SURGICAL DEVICE\",\n          \"DBS LEAD\",\n          \"HEART-VALVE, NON-ALLOGRAFT TISSUE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DEVICE_REPORT_PRODUCT_CODE\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 763,\n        \"samples\": [\n          \"MJO\",\n          \"IZQ\",\n          \"DTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IMPLANT_FLAG\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"AIZU OLYMPUS CO., LTD.\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"COMBINATION_PRODUCT_FLAG\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"N\",\n          \"Y\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UDI-DI\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5736,\n        \"samples\": [\n          \"00613994740915\",\n          \"00813132023058\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"device_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4001,\n        \"min\": 0,\n        \"max\": 13859,\n        \"num_unique_values\": 13860,\n        \"samples\": [\n          7359,\n          3773\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_for_embed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6422,\n        \"samples\": [\n          \"CAPSURE SENSE LEAD MRI SURESCAN ; PERMANENT PACEMAKER ELECTRODE ; product code DTB\",\n          \"ZEPHYR DR ; PULSE GENERATOR, PERMANENT, IMPLANTABLE ; product code NVZ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# read device data\n",
        "dtype_all = \"string\"\n",
        "device_df = pd.read_csv(\n",
        "    device_fp,\n",
        "    sep=\"|\",\n",
        "    dtype=dtype_all,\n",
        "    encoding=\"latin-1\",\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"skip\",\n",
        ")\n",
        "\n",
        "# initial clean up\n",
        "device_df = device_df.applymap(lambda x: x.strip() if isinstance(x, str) else x).fillna(\"\")\n",
        "\n",
        "# date parse\n",
        "for col in [\"DATE_RECEIVED\", \"EXPIRATION_DATE_OF_DEVICE\", \"DATE_RETURNED_TO_MANUFACTURER\"]:\n",
        "    if col in device_df.columns:\n",
        "        device_df[col] = pd.to_datetime(device_df[col], errors=\"coerce\")\n",
        "\n",
        "# ensure needed columns exist\n",
        "needed = [\n",
        "    \"MDR_REPORT_KEY\",\"DEVICE_EVENT_KEY\",\"IMPLANT_FLAG\",\"DATE_REMOVED_FLAG\",\"DEVICE_SEQUENCE_NO\",\"DATE_RECEIVED\",\n",
        "    \"BRAND_NAME\",\"GENERIC_NAME\",\"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"CATALOG_NUMBER\",\"LOT_NUMBER\",\"OTHER_ID_NUMBER\",\n",
        "    \"DEVICE_REPORT_PRODUCT_CODE\",\"DEVICE_OPERATOR\",\"DEVICE_EVALUATED_BY_MANUFACTUR\",\"COMBINATION_PRODUCT_FLAG\",\n",
        "    \"UDI-DI\",\"UDI-PUBLIC\",\n",
        "]\n",
        "present = [c for c in needed if c in device_df.columns]\n",
        "device_df = device_df[present].copy()\n",
        "\n",
        "# drop rows missing core IDs\n",
        "device_df = device_df[\n",
        "    device_df[\"MANUFACTURER_D_NAME\"].ne(\"\") &\n",
        "    device_df[\"MODEL_NUMBER\"].ne(\"\")\n",
        "]\n",
        "\n",
        "# build device_key\n",
        "for add in [\"CATALOG_NUMBER\", \"UDI-DI\"]:\n",
        "    if add not in device_df.columns:\n",
        "        device_df[add] = \"\"\n",
        "\n",
        "device_df[\"device_key\"] = (\n",
        "    device_df[\"MANUFACTURER_D_NAME\"] + \"||\" +\n",
        "    device_df[\"MODEL_NUMBER\"]       + \"||\" +\n",
        "    device_df[\"CATALOG_NUMBER\"]     + \"||\" +\n",
        "    device_df[\"UDI-DI\"]\n",
        ").astype(\"string\")\n",
        "\n",
        "# build device_keys with device_id\n",
        "device_keys = device_df[[\n",
        "    \"device_key\",\"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"CATALOG_NUMBER\",\"BRAND_NAME\",\n",
        "    \"GENERIC_NAME\",\"DEVICE_REPORT_PRODUCT_CODE\",\"IMPLANT_FLAG\",\"COMBINATION_PRODUCT_FLAG\",\"UDI-DI\"\n",
        "]].drop_duplicates().reset_index(drop=True)\n",
        "device_keys[\"device_id\"] = np.arange(len(device_keys), dtype=np.int64)\n",
        "\n",
        "# extract device text for embedding at device level\n",
        "def make_device_text(row):\n",
        "    parts = []\n",
        "    if isinstance(row[\"BRAND_NAME\"], str) and row[\"BRAND_NAME\"]:\n",
        "        parts.append(row[\"BRAND_NAME\"])\n",
        "    if isinstance(row[\"GENERIC_NAME\"], str) and row[\"GENERIC_NAME\"]:\n",
        "        parts.append(row[\"GENERIC_NAME\"])\n",
        "    if isinstance(row[\"DEVICE_REPORT_PRODUCT_CODE\"], str) and row[\"DEVICE_REPORT_PRODUCT_CODE\"]:\n",
        "        parts.append(f\"product code {row['DEVICE_REPORT_PRODUCT_CODE']}\")\n",
        "    return \" ; \".join(parts) if parts else \"unknown medical device\"\n",
        "\n",
        "if \"text_for_embed\" not in device_keys.columns:\n",
        "    device_keys[\"text_for_embed\"] = device_keys.apply(make_device_text, axis=1)\n",
        "\n",
        "# sort by device_id\n",
        "device_texts = device_keys.sort_values(\"device_id\")[\"text_for_embed\"].tolist()\n",
        "\n",
        "# validation\n",
        "device_df.head(5)\n",
        "device_keys.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load and clean the FOI Text 2024 file"
      ],
      "metadata": {
        "id": "UOkgGhuc7r39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FOI text file\n",
        "foi_df = pd.read_csv(foi_fp, sep='|',\n",
        "                        dtype=dtype_all,\n",
        "                        encoding='latin-1',\n",
        "                        engine='python',\n",
        "                        on_bad_lines='skip') #skip bad rows for now\n",
        "# Basic cleanup\n",
        "foi_df = foi_df.applymap(lambda x: x.strip() if isinstance(x, str) else x).fillna(\"\")\n",
        "\n",
        "# Filter to device narrative text (TEXT_TYPE_CODE == 'D')\n",
        "foi_dev = foi_df[foi_df[\"TEXT_TYPE_CODE\"] == \"D\"].copy()\n",
        "\n",
        "# Aggregate all device narratives per report into a single text blob\n",
        "report_text = (\n",
        "    foi_dev.groupby(\"MDR_REPORT_KEY\")[\"FOI_TEXT\"]\n",
        "    .apply(lambda s: \" \".join(s.tolist()))\n",
        "    .reset_index()\n",
        "    .rename(columns={\"MDR_REPORT_KEY\": \"MDR_REPORT_KEY\",\n",
        "                     \"FOI_TEXT\": \"REPORT_FOI_TEXT\"})\n",
        ")\n",
        "\n",
        "# Join onto existing device_df\n",
        "device_df = device_df.merge(report_text, on=\"MDR_REPORT_KEY\", how=\"inner\")\n",
        "device_df[\"REPORT_FOI_TEXT\"] = device_df[\"REPORT_FOI_TEXT\"].fillna(\"\")\n",
        "\n",
        "#validation\n",
        "device_df.head(5)"
      ],
      "metadata": {
        "id": "McbIMmwi7zap",
        "outputId": "f92dd264-11b2-40d3-bf18-58e614352db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-314965748.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  foi_df = foi_df.applymap(lambda x: x.strip() if isinstance(x, str) else x).fillna(\"\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  MDR_REPORT_KEY DEVICE_EVENT_KEY IMPLANT_FLAG DATE_REMOVED_FLAG  \\\n",
              "0       18423065                                                   \n",
              "1       18423066                                                   \n",
              "2       18423067                                                   \n",
              "3       18423068                                                   \n",
              "4       18423069                                                   \n",
              "\n",
              "  DEVICE_SEQUENCE_NO DATE_RECEIVED                           BRAND_NAME  \\\n",
              "0                  1    2024-01-01       EVIS EXERA II COLONOVIDEOSCOPE   \n",
              "1                  1    2024-01-01     EVIS EXERA III BRONCHOVIDEOSCOPE   \n",
              "2                  1    2024-01-01  EVIS LUCERA ELITE BRONCHOVIDEOSCOPE   \n",
              "3                  1    2024-01-01                            ENDOWRIST   \n",
              "4                  1    2024-01-01                                  ION   \n",
              "\n",
              "                  GENERIC_NAME      MANUFACTURER_D_NAME MODEL_NUMBER  ...  \\\n",
              "0             COLONOVIDEOSCOPE   AIZU OLYMPUS CO., LTD.    CF-H180AL  ...   \n",
              "1            BRONCHOVIDEOSCOPE   AIZU OLYMPUS CO., LTD.     BF-XP190  ...   \n",
              "2            BRONCHOVIDEOSCOPE   AIZU OLYMPUS CO., LTD.      BF-Q290  ...   \n",
              "3  FENESTRATED BIPOLAR FORCEPS  INTUITIVE SURGICAL, INC    470205-17  ...   \n",
              "4                 VISION PROBE  INTUITIVE SURGICAL, INC    490206-01  ...   \n",
              "\n",
              "       LOT_NUMBER OTHER_ID_NUMBER DEVICE_REPORT_PRODUCT_CODE DEVICE_OPERATOR  \\\n",
              "0                                                        FDF             0HP   \n",
              "1                                                        EOQ             0HP   \n",
              "2                                                        EOQ             0HP   \n",
              "3  N10210510 0025                                        NAY             0HP   \n",
              "4  S14230914 0017                                        EOQ             0HP   \n",
              "\n",
              "  DEVICE_EVALUATED_BY_MANUFACTUR COMBINATION_PRODUCT_FLAG          UDI-DI  \\\n",
              "0                              Y                        N  04953170202339   \n",
              "1                              Y                        N  04953170342134   \n",
              "2                              Y                        N                   \n",
              "3                              Y                        N  00886874112359   \n",
              "4                              Y                        N  00886874121184   \n",
              "\n",
              "                                  UDI-PUBLIC  \\\n",
              "0                             04953170202339   \n",
              "1                             04953170342134   \n",
              "2                                              \n",
              "3  (01)00886874112359(11)210506(10)N10210510   \n",
              "4            (01)00886874121184(10)S14230914   \n",
              "\n",
              "                                          device_key  \\\n",
              "0  AIZU OLYMPUS CO., LTD.||CF-H180AL||||049531702...   \n",
              "1  AIZU OLYMPUS CO., LTD.||BF-XP190||||0495317034...   \n",
              "2                AIZU OLYMPUS CO., LTD.||BF-Q290||||   \n",
              "3  INTUITIVE SURGICAL, INC||470205-17||470205||00...   \n",
              "4  INTUITIVE SURGICAL, INC||490206-01||490206||00...   \n",
              "\n",
              "                                     REPORT_FOI_TEXT  \n",
              "0  A USER FACILITY RETURNED THE OLYMPUS ASSET, TH...  \n",
              "1  THE CUSTOMER REPORTED TO OLYMPUS, THE EVIS EXE...  \n",
              "2  THE CUSTOMER REPORTED TO OLYMPUS THE BRONCHOVI...  \n",
              "3  IT WAS REPORTED THAT DURING A DA VINCI-ASSISTE...  \n",
              "4  IT WAS REPORTED THAT DURING AN ION ENDOBRONCHI...  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3cbd3cb-5a09-41c2-8814-f50448c95032\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MDR_REPORT_KEY</th>\n",
              "      <th>DEVICE_EVENT_KEY</th>\n",
              "      <th>IMPLANT_FLAG</th>\n",
              "      <th>DATE_REMOVED_FLAG</th>\n",
              "      <th>DEVICE_SEQUENCE_NO</th>\n",
              "      <th>DATE_RECEIVED</th>\n",
              "      <th>BRAND_NAME</th>\n",
              "      <th>GENERIC_NAME</th>\n",
              "      <th>MANUFACTURER_D_NAME</th>\n",
              "      <th>MODEL_NUMBER</th>\n",
              "      <th>...</th>\n",
              "      <th>LOT_NUMBER</th>\n",
              "      <th>OTHER_ID_NUMBER</th>\n",
              "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
              "      <th>DEVICE_OPERATOR</th>\n",
              "      <th>DEVICE_EVALUATED_BY_MANUFACTUR</th>\n",
              "      <th>COMBINATION_PRODUCT_FLAG</th>\n",
              "      <th>UDI-DI</th>\n",
              "      <th>UDI-PUBLIC</th>\n",
              "      <th>device_key</th>\n",
              "      <th>REPORT_FOI_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18423065</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>EVIS EXERA II COLONOVIDEOSCOPE</td>\n",
              "      <td>COLONOVIDEOSCOPE</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.</td>\n",
              "      <td>CF-H180AL</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>FDF</td>\n",
              "      <td>0HP</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>04953170202339</td>\n",
              "      <td>04953170202339</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.||CF-H180AL||||049531702...</td>\n",
              "      <td>A USER FACILITY RETURNED THE OLYMPUS ASSET, TH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18423066</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>EVIS EXERA III BRONCHOVIDEOSCOPE</td>\n",
              "      <td>BRONCHOVIDEOSCOPE</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.</td>\n",
              "      <td>BF-XP190</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>EOQ</td>\n",
              "      <td>0HP</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>04953170342134</td>\n",
              "      <td>04953170342134</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.||BF-XP190||||0495317034...</td>\n",
              "      <td>THE CUSTOMER REPORTED TO OLYMPUS, THE EVIS EXE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18423067</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>EVIS LUCERA ELITE BRONCHOVIDEOSCOPE</td>\n",
              "      <td>BRONCHOVIDEOSCOPE</td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.</td>\n",
              "      <td>BF-Q290</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>EOQ</td>\n",
              "      <td>0HP</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>AIZU OLYMPUS CO., LTD.||BF-Q290||||</td>\n",
              "      <td>THE CUSTOMER REPORTED TO OLYMPUS THE BRONCHOVI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18423068</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>ENDOWRIST</td>\n",
              "      <td>FENESTRATED BIPOLAR FORCEPS</td>\n",
              "      <td>INTUITIVE SURGICAL, INC</td>\n",
              "      <td>470205-17</td>\n",
              "      <td>...</td>\n",
              "      <td>N10210510 0025</td>\n",
              "      <td></td>\n",
              "      <td>NAY</td>\n",
              "      <td>0HP</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>00886874112359</td>\n",
              "      <td>(01)00886874112359(11)210506(10)N10210510</td>\n",
              "      <td>INTUITIVE SURGICAL, INC||470205-17||470205||00...</td>\n",
              "      <td>IT WAS REPORTED THAT DURING A DA VINCI-ASSISTE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18423069</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>ION</td>\n",
              "      <td>VISION PROBE</td>\n",
              "      <td>INTUITIVE SURGICAL, INC</td>\n",
              "      <td>490206-01</td>\n",
              "      <td>...</td>\n",
              "      <td>S14230914 0017</td>\n",
              "      <td></td>\n",
              "      <td>EOQ</td>\n",
              "      <td>0HP</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>00886874121184</td>\n",
              "      <td>(01)00886874121184(10)S14230914</td>\n",
              "      <td>INTUITIVE SURGICAL, INC||490206-01||490206||00...</td>\n",
              "      <td>IT WAS REPORTED THAT DURING AN ION ENDOBRONCHI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3cbd3cb-5a09-41c2-8814-f50448c95032')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3cbd3cb-5a09-41c2-8814-f50448c95032 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3cbd3cb-5a09-41c2-8814-f50448c95032');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-87201b5d-d93f-4fdf-97db-14c4d766e491\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-87201b5d-d93f-4fdf-97db-14c4d766e491')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-87201b5d-d93f-4fdf-97db-14c4d766e491 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "device_df"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique reports\n",
        "report = pd.DataFrame({\n",
        "    \"MDR_REPORT_KEY\": device_df[\"MDR_REPORT_KEY\"].unique()\n",
        "}).reset_index(drop=True)\n",
        "report[\"report_id\"] = np.arange(len(report), dtype=np.int64)\n",
        "\n",
        "# Aggregate FOI text per report\n",
        "report_text = (\n",
        "    device_df.groupby(\"MDR_REPORT_KEY\")[\"REPORT_FOI_TEXT\"]\n",
        "    .apply(lambda s: \" \".join(s.astype(str).tolist()))\n",
        "    .reset_index()\n",
        "    .rename(columns={\"REPORT_FOI_TEXT\": \"REPORT_FOI_TEXT_AGG\"})\n",
        ")\n",
        "\n",
        "# Merge aggregated FOI text onto report table\n",
        "report = report.merge(report_text, on=\"MDR_REPORT_KEY\", how=\"left\")\n",
        "report[\"REPORT_FOI_TEXT_AGG\"] = report[\"REPORT_FOI_TEXT_AGG\"].fillna(\"\")\n",
        "\n",
        "# sanity check\n",
        "report.head(5)"
      ],
      "metadata": {
        "id": "VgkNPcsSF2SY",
        "outputId": "61030211-ebbc-4312-d110-524ee61f9db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  MDR_REPORT_KEY  report_id                                REPORT_FOI_TEXT_AGG\n",
              "0       18423065          0  A USER FACILITY RETURNED THE OLYMPUS ASSET, TH...\n",
              "1       18423066          1  THE CUSTOMER REPORTED TO OLYMPUS, THE EVIS EXE...\n",
              "2       18423067          2  THE CUSTOMER REPORTED TO OLYMPUS THE BRONCHOVI...\n",
              "3       18423068          3  IT WAS REPORTED THAT DURING A DA VINCI-ASSISTE...\n",
              "4       18423069          4  IT WAS REPORTED THAT DURING AN ION ENDOBRONCHI..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8d4958e-8463-4338-8294-6194c2996e70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MDR_REPORT_KEY</th>\n",
              "      <th>report_id</th>\n",
              "      <th>REPORT_FOI_TEXT_AGG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18423065</td>\n",
              "      <td>0</td>\n",
              "      <td>A USER FACILITY RETURNED THE OLYMPUS ASSET, TH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18423066</td>\n",
              "      <td>1</td>\n",
              "      <td>THE CUSTOMER REPORTED TO OLYMPUS, THE EVIS EXE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18423067</td>\n",
              "      <td>2</td>\n",
              "      <td>THE CUSTOMER REPORTED TO OLYMPUS THE BRONCHOVI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18423068</td>\n",
              "      <td>3</td>\n",
              "      <td>IT WAS REPORTED THAT DURING A DA VINCI-ASSISTE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18423069</td>\n",
              "      <td>4</td>\n",
              "      <td>IT WAS REPORTED THAT DURING AN ION ENDOBRONCHI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8d4958e-8463-4338-8294-6194c2996e70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8d4958e-8463-4338-8294-6194c2996e70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8d4958e-8463-4338-8294-6194c2996e70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bdcc882a-ddfc-4334-89fc-966f5a2fd8a0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdcc882a-ddfc-4334-89fc-966f5a2fd8a0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bdcc882a-ddfc-4334-89fc-966f5a2fd8a0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Sentence Embeddings (Sentence BERT) using FOI Report Text"
      ],
      "metadata": {
        "id": "ecIoQBHo-8-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device_cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare texts in report_id order\n",
        "report_texts = (\n",
        "    report.sort_values(\"report_id\")[\"REPORT_FOI_TEXT_AGG\"]\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "# Load SBERT and encode\n",
        "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "sbert_model = sbert_model.to(device_cuda)\n",
        "sbert_model.eval()\n",
        "\n",
        "batch_size = 256\n",
        "embs = []\n",
        "\n",
        "for i in range(0, len(report_texts), batch_size):\n",
        "    batch = report_texts[i:i+batch_size]\n",
        "    with torch.no_grad():\n",
        "        emb = sbert_model.encode(\n",
        "            batch,\n",
        "            convert_to_tensor=True,\n",
        "            device=sbert_model.device,\n",
        "            show_progress_bar=False,\n",
        "        )  # [B, 384]\n",
        "    embs.append(emb.cpu())\n",
        "\n",
        "report_text_emb = torch.cat(embs, dim=0)  # [num_reports, 384]\n",
        "assert report_text_emb.size(0) == len(report)\n",
        "print(\"report_text_emb:\", report_text_emb.shape)"
      ],
      "metadata": {
        "id": "pUsoFe57_ENM",
        "outputId": "82c1bfca-438f-4869-ffdb-20c2ae324d1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report_text_emb: torch.Size([106497, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri0qJeapLtNk"
      },
      "source": [
        "\n",
        "\n",
        "### 3) split of the data within Jan 2024 MDR File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w4kX1JkExFQ",
        "outputId": "a2f52478-8817-4f53-ae34-48c9812ecb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPORT_DATE range: 2024-01-01 00:00:00 -> 2024-01-30 00:00:00\n",
            "split\n",
            "train    73387\n",
            "val      21653\n",
            "test     11457\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1) Build report-level dates and splits\n",
        "report_dates = (\n",
        "    device_df.groupby(\"MDR_REPORT_KEY\")[\"DATE_RECEIVED\"]\n",
        "    .min()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"DATE_RECEIVED\": \"REPORT_DATE\"})\n",
        ")\n",
        "\n",
        "min_date = report_dates[\"REPORT_DATE\"].min()\n",
        "max_date = report_dates[\"REPORT_DATE\"].max()\n",
        "print(\"REPORT_DATE range:\", min_date, \"->\", max_date)\n",
        "\n",
        "train_cutoff = min_date + (max_date - min_date) * 0.70\n",
        "val_cutoff   = min_date + (max_date - min_date) * 0.85\n",
        "\n",
        "def assign_report_split(ts):\n",
        "    if pd.isna(ts):\n",
        "        return \"drop\"\n",
        "    if ts <= train_cutoff:\n",
        "        return \"train\"\n",
        "    elif ts <= val_cutoff:\n",
        "        return \"val\"\n",
        "    else:\n",
        "        return \"test\"\n",
        "\n",
        "report_dates[\"split\"] = report_dates[\"REPORT_DATE\"].apply(assign_report_split)\n",
        "report_dates = report_dates[report_dates[\"split\"] != \"drop\"]\n",
        "\n",
        "# 2) Merge splits back into device_df\n",
        "device_df = device_df.merge(\n",
        "    report_dates[[\"MDR_REPORT_KEY\", \"split\"]],\n",
        "    on=\"MDR_REPORT_KEY\",\n",
        "    how=\"inner\",\n",
        ")\n",
        "\n",
        "print(device_df[\"split\"].value_counts())\n",
        "train_rows = device_df[device_df[\"split\"] == \"train\"].copy()\n",
        "val_rows   = device_df[device_df[\"split\"] == \"val\"].copy()\n",
        "test_rows  = device_df[device_df[\"split\"] == \"test\"].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iUcxOpvQ188"
      },
      "source": [
        "### Choose a target relation for the Link Prediction Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baPGwaruPc8m",
        "outputId": "bbd14c92-e157-4c2a-f8e7-bc3bcc310fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target link prediction relation: ('report', 'mentions', 'device')\n"
          ]
        }
      ],
      "source": [
        "# Choose target relation to predict from the AVAILABLE_RELS list:\n",
        "\n",
        "AVAILABLE_RELS = [\n",
        "    (\"report\",\"mentions\",\"device\"),\n",
        "    (\"manufacturer\",\"makes\",\"device\"),\n",
        "    (\"event\",\"involves\",\"device\"),\n",
        "]\n",
        "\n",
        "TARGET_REL = (\"report\",\"mentions\",\"device\")\n",
        "assert TARGET_REL in AVAILABLE_RELS, f\"{TARGET_REL} not in available relations\"\n",
        "REL = TARGET_REL\n",
        "print(\"Target link prediction relation:\", REL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow94O3DqkijE"
      },
      "source": [
        "## 4) create node tables + id maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dR5f6fm5kk4h"
      },
      "outputs": [],
      "source": [
        "## 4) create node tables + id maps (post temporal split)\n",
        "\n",
        "# manufacturers\n",
        "manuf = pd.DataFrame({\"manufacturer_name\": device_df[\"MANUFACTURER_D_NAME\"].unique()})\n",
        "manuf[\"manufacturer_id\"] = np.arange(len(manuf), dtype=np.int64)\n",
        "\n",
        "# devices (unique by canonical device_key + metadata)\n",
        "device_keys = device_df[[\n",
        "    \"device_key\",\"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"CATALOG_NUMBER\",\"BRAND_NAME\",\n",
        "    \"GENERIC_NAME\",\"DEVICE_REPORT_PRODUCT_CODE\",\"IMPLANT_FLAG\",\"COMBINATION_PRODUCT_FLAG\",\"UDI-DI\"\n",
        "]].drop_duplicates()\n",
        "device_keys = device_keys.reset_index(drop=True)\n",
        "device_keys[\"device_id\"] = np.arange(len(device_keys), dtype=np.int64)\n",
        "\n",
        "# reports\n",
        "report = pd.DataFrame({\"report_key\": device_df[\"MDR_REPORT_KEY\"].unique()})\n",
        "report[\"report_id\"] = np.arange(len(report), dtype=np.int64)\n",
        "\n",
        "# events (if available)\n",
        "if \"DEVICE_EVENT_KEY\" in device_df.columns and device_df[\"DEVICE_EVENT_KEY\"].ne(\"\").any():\n",
        "    event = pd.DataFrame({\"event_key\": device_df[\"DEVICE_EVENT_KEY\"].replace(\"\", np.nan).dropna().unique()})\n",
        "else:\n",
        "    event = pd.DataFrame({\"event_key\": [\"UNK\"]})\n",
        "event[\"event_id\"] = np.arange(len(event), dtype=np.int64)\n",
        "\n",
        "# map back to main df\n",
        "device_df = device_df.merge(manuf, left_on=\"MANUFACTURER_D_NAME\",\n",
        "                            right_on=\"manufacturer_name\", how=\"left\")\n",
        "\n",
        "device_df = device_df.merge(device_keys[[\"device_key\",\"device_id\"]],\n",
        "                            on=\"device_key\", how=\"left\")\n",
        "\n",
        "device_df = device_df.merge(report, left_on=\"MDR_REPORT_KEY\",\n",
        "                            right_on=\"report_key\", how=\"left\")\n",
        "\n",
        "device_df[\"event_key\"] = device_df[\"DEVICE_EVENT_KEY\"].replace(\"\", \"UNK\")\n",
        "device_df = device_df.merge(event, on=\"event_key\", how=\"left\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIBGYF7Vr62g"
      },
      "source": [
        "## 5) build edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84epXDylksgo",
        "outputId": "f005aad4-3d3e-462b-ce87-dd834495cf60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report->device edges: train 190783 val 50992 test 28319\n"
          ]
        }
      ],
      "source": [
        "def to_edge_index(df):\n",
        "    if df.empty:\n",
        "        return torch.empty(2, 0, dtype=torch.long)\n",
        "    return torch.tensor(df.values.T, dtype=torch.long)\n",
        "\n",
        "# global edges\n",
        "md = device_df[[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "edge_manuf_device = torch.tensor(md[[\"manufacturer_id\",\"device_id\"]].values.T,\n",
        "                                 dtype=torch.long)\n",
        "\n",
        "ed = device_df[[\"event_id\",\"device_id\"]].drop_duplicates()\n",
        "edge_event_device = torch.tensor(ed[[\"event_id\",\"device_id\"]].values.T,\n",
        "                                 dtype=torch.long)\n",
        "\n",
        "temporal_edges = {}\n",
        "\n",
        "#build edges corresponding to selected target relation\n",
        "if REL == (\"report\",\"mentions\",\"device\"):\n",
        "    rd_train = device_df[device_df[\"split\"] == \"train\"][[\"report_id\",\"device_id\"]].drop_duplicates()\n",
        "    rd_val   = device_df[device_df[\"split\"] == \"val\"][[\"report_id\",\"device_id\"]].drop_duplicates()\n",
        "    rd_test  = device_df[device_df[\"split\"] == \"test\"][[\"report_id\",\"device_id\"]].drop_duplicates()\n",
        "\n",
        "    edge_report_device_train = to_edge_index(rd_train)\n",
        "    edge_report_device_val   = to_edge_index(rd_val)\n",
        "    edge_report_device_test  = to_edge_index(rd_test)\n",
        "\n",
        "    temporal_edges[\"train\"] = edge_report_device_train\n",
        "    temporal_edges[\"val\"]   = edge_report_device_val\n",
        "    temporal_edges[\"test\"]  = edge_report_device_test\n",
        "\n",
        "    print(\"report->device edges:\",\n",
        "          \"train\", edge_report_device_train.size(1),\n",
        "          \"val\",   edge_report_device_val.size(1),\n",
        "          \"test\",  edge_report_device_test.size(1))\n",
        "\n",
        "elif REL == (\"manufacturer\",\"makes\",\"device\"):\n",
        "    md_train = device_df[device_df[\"split\"] == \"train\"][[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "    md_val   = device_df[device_df[\"split\"] == \"val\"][[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "    md_test  = device_df[device_df[\"split\"] == \"test\"][[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "\n",
        "    edge_manuf_device_train = to_edge_index(md_train)\n",
        "    edge_manuf_device_val   = to_edge_index(md_val)\n",
        "    edge_manuf_device_test  = to_edge_index(md_test)\n",
        "\n",
        "    temporal_edges[\"train\"] = edge_manuf_device_train\n",
        "    temporal_edges[\"val\"]   = edge_manuf_device_val\n",
        "    temporal_edges[\"test\"]  = edge_manuf_device_test\n",
        "\n",
        "    print(\"manufacturer->device edges:\",\n",
        "          \"train\", edge_manuf_device_train.size(1),\n",
        "          \"val\",   edge_manuf_device_val.size(1),\n",
        "          \"test\",  edge_manuf_device_test.size(1))\n",
        "\n",
        "elif REL == (\"event\",\"involves\",\"device\"):\n",
        "    ed_train = device_df[device_df[\"split\"] == \"train\"][[\"event_id\",\"device_id\"]].drop_duplicates()\n",
        "    ed_val   = device_df[device_df[\"split\"] == \"val\"][[\"event_id\",\"device_id\"]].drop_duplicates()\n",
        "    ed_test  = device_df[device_df[\"split\"] == \"test\"][[\"event_id\",\"device_id\"]].drop_duplicates()\n",
        "\n",
        "    edge_event_device_train = to_edge_index(ed_train)\n",
        "    edge_event_device_val   = to_edge_index(ed_val)\n",
        "    edge_event_device_test  = to_edge_index(ed_test)\n",
        "\n",
        "    temporal_edges[\"train\"] = edge_event_device_train\n",
        "    temporal_edges[\"val\"]   = edge_event_device_val\n",
        "    temporal_edges[\"test\"]  = edge_event_device_test\n",
        "\n",
        "    print(\"event->device edges:\",\n",
        "          \"train\", edge_event_device_train.size(1),\n",
        "          \"val\",   edge_event_device_val.size(1),\n",
        "          \"test\",  edge_event_device_test.size(1))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported TARGET_REL={REL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1Rp7hekun9"
      },
      "source": [
        "## 6) Embedded features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QvyN8QfMkyMa"
      },
      "outputs": [],
      "source": [
        "# categorical vocabularies (from device table)\n",
        "brand_vocab   = pd.Series(device_keys[\"BRAND_NAME\"].astype(\"category\").cat.categories)\n",
        "generic_vocab = pd.Series(device_keys[\"GENERIC_NAME\"].astype(\"category\").cat.categories)\n",
        "prod_vocab    = pd.Series(device_keys[\"DEVICE_REPORT_PRODUCT_CODE\"].astype(\"category\").cat.categories)\n",
        "\n",
        "brand2ix   = {b: i for i, b in enumerate(brand_vocab)}\n",
        "generic2ix = {g: i for i, g in enumerate(generic_vocab)}\n",
        "prod2ix    = {p: i for i, p in enumerate(prod_vocab)}\n",
        "\n",
        "# map to indices using device_keys (one row per device_id)\n",
        "dev_feat = pd.DataFrame(index=device_keys[\"device_id\"])\n",
        "dev_feat[\"brand_ix\"]   = device_keys[\"BRAND_NAME\"].map(brand2ix).fillna(-1).astype(int)\n",
        "dev_feat[\"generic_ix\"] = device_keys[\"GENERIC_NAME\"].map(generic2ix).fillna(-1).astype(int)\n",
        "dev_feat[\"prod_ix\"]    = device_keys[\"DEVICE_REPORT_PRODUCT_CODE\"].map(prod2ix).fillna(-1).astype(int)\n",
        "\n",
        "# flag indices\n",
        "flag_map = {\"Y\": 1, \"N\": 0, \"\": 0}\n",
        "dev_feat[\"implant_flag\"] = device_keys[\"IMPLANT_FLAG\"].map(lambda x: flag_map.get(str(x).upper(), 0)).astype(int)\n",
        "dev_feat[\"combo_flag\"]   = device_keys[\"COMBINATION_PRODUCT_FLAG\"].map(lambda x: flag_map.get(str(x).upper(), 0)).astype(int)\n",
        "\n",
        "# tensors: device has 5 feature columns, others just ID indices\n",
        "device_x = torch.tensor(\n",
        "    dev_feat[[\"brand_ix\",\"generic_ix\",\"prod_ix\",\"implant_flag\",\"combo_flag\"]].values,\n",
        "    dtype=torch.long,\n",
        ")\n",
        "manuf_x  = torch.arange(len(manuf),  dtype=torch.long).unsqueeze(1)\n",
        "report_x = torch.arange(len(report), dtype=torch.long).unsqueeze(1)\n",
        "event_x  = torch.arange(len(event),  dtype=torch.long).unsqueeze(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWZ_-V2MvdG"
      },
      "source": [
        "#### 7) Set bounds on feature indices and split for train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t02cEAWxFv95",
        "outputId": "dd545118-5615-4f58-8bd6-ed9564182513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rows per split: train 190783 val 50992 test 28319\n"
          ]
        }
      ],
      "source": [
        "# bounds on device feature indices\n",
        "B = max(1, len(brand2ix))\n",
        "G = max(1, len(generic2ix))\n",
        "P = max(1, len(prod2ix))\n",
        "\n",
        "if device_x.numel() > 0:\n",
        "    device_x[:, 0] = torch.clamp(device_x[:, 0], 0, B-1)  # brand_ix\n",
        "    device_x[:, 1] = torch.clamp(device_x[:, 1], 0, G-1)  # generic_ix\n",
        "    device_x[:, 2] = torch.clamp(device_x[:, 2], 0, P-1)  # prod_ix\n",
        "    device_x[:, 3] = torch.clamp(device_x[:, 3], 0, 1)    # implant_flag\n",
        "    device_x[:, 4] = torch.clamp(device_x[:, 4], 0, 1)    # combo_flag\n",
        "\n",
        "\n",
        "train_rows = device_df[device_df[\"split\"] == \"train\"].copy()\n",
        "val_rows   = device_df[device_df[\"split\"] == \"val\"].copy()\n",
        "test_rows  = device_df[device_df[\"split\"] == \"test\"].copy()\n",
        "\n",
        "print(\"rows per split:\",\n",
        "      \"train\", len(train_rows),\n",
        "      \"val\",   len(val_rows),\n",
        "      \"test\",  len(test_rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2otYR3ISsMVR"
      },
      "source": [
        "## 8) build HeteroData + split for link prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIwFVrwLsN2u",
        "outputId": "5e1a381c-0c05-4450-e14c-07c6d0acdb2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edge counts report->device: train 190783 val 50992 test 28319\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# Node counts from node tables\n",
        "num_reports = len(report)          # report_id 0..num_reports-1\n",
        "num_devices = len(device_keys)     # device_id 0..num_devices-1\n",
        "num_mfrs    = len(manuf)           # manufacturer_id 0..num_mfrs-1\n",
        "\n",
        "data[\"report\"].num_nodes       = num_reports\n",
        "data[\"device\"].num_nodes       = num_devices\n",
        "data[\"manufacturer\"].num_nodes = num_mfrs\n",
        "\n",
        "def build_edge_index(df, src_col, dst_col):\n",
        "    if df.empty:\n",
        "        return torch.empty(2, 0, dtype=torch.long)\n",
        "    src = torch.as_tensor(df[src_col].values, dtype=torch.long)\n",
        "    dst = torch.as_tensor(df[dst_col].values, dtype=torch.long)\n",
        "    return torch.stack([src, dst], dim=0)\n",
        "\n",
        "if REL == (\"report\",\"mentions\",\"device\"):\n",
        "    train_edges_df = train_rows[[\"report_id\",\"device_id\"]].drop_duplicates()\n",
        "    val_edges_df   = val_rows[[\"report_id\",\"device_id\"]].drop_duplicates()\n",
        "    test_edges_df  = test_rows[[\"report_id\",\"device_id\"]].drop_duplicates()\n",
        "    src_nt, dst_nt = \"report\", \"device\"\n",
        "    src_col, dst_col = \"report_id\", \"device_id\"\n",
        "\n",
        "elif REL == (\"manufacturer\",\"makes\",\"device\"):\n",
        "    train_edges_df = mfr_train_rows[[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "    val_edges_df   = mfr_val_rows[[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "    test_edges_df  = mfr_test_rows[[\"manufacturer_id\",\"device_id\"]].drop_duplicates()\n",
        "    src_nt, dst_nt = \"manufacturer\", \"device\"\n",
        "    src_col, dst_col = \"manufacturer_id\", \"device_id\"\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported TARGET_REL={REL}\")\n",
        "\n",
        "edge_train = build_edge_index(train_edges_df, src_col, dst_col)\n",
        "edge_val   = build_edge_index(val_edges_df,   src_col, dst_col)\n",
        "edge_test  = build_edge_index(test_edges_df,  src_col, dst_col)\n",
        "\n",
        "# Set target relation edges on the graph\n",
        "data[src_nt, REL[1], dst_nt].edge_index = edge_train\n",
        "data[dst_nt, f\"rev_{REL[1]}\", src_nt].edge_index = edge_train.flip(0)\n",
        "\n",
        "# manufacturer -> device\n",
        "mfr_dev_all = build_edge_index(\n",
        "    device_df[[\"manufacturer_id\",\"device_id\"]].drop_duplicates(),\n",
        "    \"manufacturer_id\", \"device_id\"\n",
        ")\n",
        "data[\"manufacturer\", \"makes\", \"device\"].edge_index = mfr_dev_all\n",
        "data[\"device\", \"made_by\", \"manufacturer\"].edge_index = mfr_dev_all.flip(0)\n",
        "\n",
        "temporal_edges = {\n",
        "    \"train\": edge_train,\n",
        "    \"val\":   edge_val,\n",
        "    \"test\":  edge_test,\n",
        "}\n",
        "\n",
        "print(f\"edge counts {REL[0]}->{REL[2]}:\",\n",
        "      \"train\", edge_train.size(1),\n",
        "      \"val\",   edge_val.size(1),\n",
        "      \"test\",  edge_test.size(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build utilities for negative edge sampling"
      ],
      "metadata": {
        "id": "OxZNsSeZysDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_reports = data[\"report\"].num_nodes\n",
        "num_devices = data[\"device\"].num_nodes\n",
        "\n",
        "# build a set of observed positive pairs for masking\n",
        "def build_obs_set(edge_index):\n",
        "    # edge_index: [2, E]\n",
        "    return set(map(tuple, edge_index.t().tolist()))\n",
        "\n",
        "pos_train = edge_train\n",
        "pos_val   = edge_val\n",
        "pos_test  = edge_test\n",
        "\n",
        "obs_train = build_obs_set(pos_train)\n",
        "obs_val   = build_obs_set(pos_val)\n",
        "obs_test  = build_obs_set(pos_test)\n",
        "\n",
        "def sample_negatives_masked(num_src, num_dst, num_neg, obs_pairs, device):\n",
        "    neg = []\n",
        "    while len(neg) < num_neg:\n",
        "        src = np.random.randint(0, num_src)\n",
        "        dst = np.random.randint(0, num_dst)\n",
        "        if (src, dst) not in obs_pairs:\n",
        "            neg.append((src, dst))\n",
        "    neg = torch.tensor(neg, dtype=torch.long, device=device).t()  # [2, num_neg]\n",
        "    return neg\n",
        "\n",
        "neg_train = sample_negatives_masked(num_reports, num_devices,\n",
        "                                    int(pos_train.size(1) * 0.5),\n",
        "                                    obs_train, pos_train.device)\n",
        "neg_val   = sample_negatives_masked(num_reports, num_devices,\n",
        "                                    int(pos_val.size(1) * 0.5),\n",
        "                                    obs_val, pos_val.device)\n",
        "neg_test  = sample_negatives_masked(num_reports, num_devices,\n",
        "                                    int(pos_test.size(1) * 0.5),\n",
        "                                    obs_test, pos_test.device)\n"
      ],
      "metadata": {
        "id": "xlK9iGD1yrAC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1APz6HDkJZig"
      },
      "source": [
        "\n",
        "\n",
        "####Sanity Checks on built graph data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swxk9pjIJcHF",
        "outputId": "7a6e13cf-89ca-4fb0-d329-c28fce3ee089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node stores\n",
            "report       num_nodes=106,497  x_rows=None\n",
            "device       num_nodes=13,850  x_rows=None\n",
            "manufacturer num_nodes=897  x_rows=None\n",
            "Edge stores\n",
            "report      -mentions    ->device        E=190,783  smax=76271<106497  dmax=13847<13850  ok=True\n",
            "device      -rev_mentions->report        E=190,783  smax=13847<13850  dmax=76271<106497  ok=True\n",
            "manufacturer-makes       ->device        E=13,850  smax=896<897  dmax=13849<13850  ok=True\n",
            "device      -made_by     ->manufacturer  E=13,850  smax=13849<13850  dmax=896<897  ok=True\n"
          ]
        }
      ],
      "source": [
        "def check_graph(data):\n",
        "    print(\"Node stores\")\n",
        "    for ntype in data.node_types:\n",
        "        assert hasattr(data[ntype], 'num_nodes'), f\"{ntype} missing num_nodes\"\n",
        "        N = int(data[ntype].num_nodes)\n",
        "        XN = None\n",
        "        if 'x' in data[ntype]:\n",
        "            XN = int(data[ntype].x.size(0))\n",
        "            ok = (XN == N)\n",
        "            print(f\"{ntype:12s} num_nodes={N:,}  x_rows={XN:,}  ok={ok}\")\n",
        "        else:\n",
        "            print(f\"{ntype:12s} num_nodes={N:,}  x_rows=None\")\n",
        "\n",
        "    print(\"Edge stores\")\n",
        "    for et in data.edge_types:\n",
        "        ei = data[et].edge_index\n",
        "        src, rel, dst = et\n",
        "        Nsrc = int(data[src].num_nodes); Ndst = int(data[dst].num_nodes)\n",
        "        smax = int(ei[0].max()) if ei.numel() else -1\n",
        "        dmax = int(ei[1].max()) if ei.numel() else -1\n",
        "        ok = (ei.numel()==0) or (smax < Nsrc and dmax < Ndst)\n",
        "        print(f\"{src:12s}-{rel:12s}->{dst:12s}  E={ei.size(1):,}  smax={smax}<{Nsrc}  dmax={dmax}<{Ndst}  ok={ok}\")\n",
        "\n",
        "check_graph(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-78XlYfZNb1y"
      },
      "source": [
        "## 8b) Attach node features to existing HeteroData and add reverse edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "KTYfUcXpJfxB"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.transforms import ToUndirected\n",
        "\n",
        "# Attach node features to the already-created data\n",
        "data[\"manufacturer\"].x = manuf_x.clone()\n",
        "data[\"report\"].x       = report_x.clone()\n",
        "data[\"event\"].x        = event_x.clone()\n",
        "data[\"device\"].x       = device_x.clone()\n",
        "\n",
        "# Make graph undirected: add reverse edge types for all relations\n",
        "data = ToUndirected()(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkZiKbz3sWhB"
      },
      "source": [
        "## 9) Use LinkNeighborLoader to build loaders that sample neighbors on the train graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "GkFTXfil612t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.multiprocessing as mp\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "\n",
        "mp.set_sharing_strategy(\"file_system\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target relation:\", REL)\n",
        "\n",
        "# Use temporal split edges as positives\n",
        "train_pos = edge_train\n",
        "val_pos   = edge_val\n",
        "test_pos  = edge_test\n",
        "\n",
        "print(f\"train={train_pos.size(1):,}  val={val_pos.size(1):,}  test={test_pos.size(1):,}\")\n",
        "\n",
        "# fanouts: sample neighbors around the target edges\n",
        "fanouts = {et: [0] for et in data.edge_types}  # 0-hop\n",
        "fanouts[REL] = [2]\n",
        "rev_rel = (REL[2], f\"rev_{REL[1]}\", REL[0])\n",
        "if rev_rel in fanouts:\n",
        "    fanouts[rev_rel] = [2]\n",
        "\n",
        "def make_loader(edge_index_pos, batch_size, shuffle):\n",
        "    return LinkNeighborLoader(\n",
        "        data=data,\n",
        "        num_neighbors=fanouts,\n",
        "        edge_label_index=(REL, edge_index_pos),\n",
        "        neg_sampling_ratio=0.5,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        replace=False,\n",
        "        pin_memory=False,\n",
        "        num_workers=0,\n",
        "        persistent_workers=False,\n",
        "    )\n",
        "\n",
        "train_loader = make_loader(train_pos, batch_size=512,  shuffle=True)\n",
        "val_loader   = make_loader(val_pos,   batch_size=1024, shuffle=False)\n",
        "test_loader  = make_loader(test_pos,  batch_size=1024, shuffle=False)\n",
        "\n",
        "# smoke test\n",
        "b = next(iter(train_loader))\n",
        "print(\"Batch node counts:\", {nt: int(b[nt].num_nodes) for nt in b.node_types})\n",
        "for nt, x in b.x_dict.items():\n",
        "    print(f\"{nt:10s} x.shape={tuple(x.shape)}\")\n"
      ],
      "metadata": {
        "id": "Z_-jLgdtNBN9",
        "outputId": "325896cc-9def-430e-c5ae-6e7713ecbd5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target relation: ('report', 'mentions', 'device')\n",
            "train=190,783  val=50,992  test=28,319\n",
            "Batch node counts: {'report': 1433, 'device': 812, 'manufacturer': 0, 'event': 0}\n",
            "report     x.shape=(1433, 1)\n",
            "device     x.shape=(812, 5)\n",
            "manufacturer x.shape=(0, 1)\n",
            "event      x.shape=(0, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Embeddings"
      ],
      "metadata": {
        "id": "6Db8oO6p-6VN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "j0QNoV23ZVdt"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torchmetrics.classification import BinaryAUROC, BinaryF1Score\n",
        "\n",
        "\n",
        "EMB = 16\n",
        "amp_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "\n",
        "class FeatureEmbedder(nn.Module):\n",
        "    def __init__(self, num_manuf, brand_sz, gen_sz, prod_sz, num_report, num_event, emb=EMB):\n",
        "        super().__init__()\n",
        "        self.manuf_emb = nn.Embedding(max(1, num_manuf), emb)\n",
        "        self.report_emb= nn.Embedding(max(1, num_report), emb)\n",
        "        self.event_emb = nn.Embedding(max(1, num_event),  emb)\n",
        "        self.brand_emb = nn.Embedding(max(1, brand_sz),  emb)\n",
        "        self.gen_emb   = nn.Embedding(max(1, gen_sz),    emb)\n",
        "        self.prod_emb  = nn.Embedding(max(1, prod_sz),   emb)\n",
        "        self.implant_emb = nn.Embedding(2, max(1, emb // 4))\n",
        "        self.combo_emb   = nn.Embedding(2, max(1, emb // 4))\n",
        "        concat_dim = emb*3 + 2*max(1, emb//4)\n",
        "        self.dev_proj = nn.Linear(concat_dim, emb)\n",
        "\n",
        "        # New: projections for SBERT-style text embeddings (384 -> emb)\n",
        "        self.device_text_proj = nn.Linear(384, emb)\n",
        "        self.report_text_proj = nn.Linear(384, emb)\n",
        "\n",
        "    def forward(self, x_dict):\n",
        "        out = {}\n",
        "\n",
        "        # Manufacturer\n",
        "        if \"manufacturer\" in x_dict:\n",
        "            out[\"manufacturer\"] = self.manuf_emb(x_dict[\"manufacturer\"].squeeze(-1))\n",
        "\n",
        "        # Report id embedding\n",
        "        if \"report\" in x_dict:\n",
        "            rep_id_emb = self.report_emb(x_dict[\"report\"].squeeze(-1))\n",
        "            if \"report_text\" in x_dict:\n",
        "                rep_txt = x_dict[\"report_text\"]          # [N_report, 384], float\n",
        "                rep_txt_emb = self.report_text_proj(rep_txt)\n",
        "                out[\"report\"] = rep_id_emb + rep_txt_emb\n",
        "            else:\n",
        "                out[\"report\"] = rep_id_emb\n",
        "\n",
        "        # Event\n",
        "        if \"event\" in x_dict:\n",
        "            out[\"event\"] = self.event_emb(x_dict[\"event\"].squeeze(-1))\n",
        "\n",
        "        # Device\n",
        "        if \"device\" in x_dict:\n",
        "            d = x_dict[\"device\"]  # [N_dev, 5] long\n",
        "            brand   = self.brand_emb(torch.clamp(d[:,0], min=0))\n",
        "            generic = self.gen_emb(torch.clamp(d[:,1],   min=0))\n",
        "            prod    = self.prod_emb(torch.clamp(d[:,2],  min=0))\n",
        "            implant = self.implant_emb(torch.clamp(d[:,3], 0, 1))\n",
        "            combo   = self.combo_emb(torch.clamp(d[:,4], 0, 1))\n",
        "            dev_cat = torch.cat([brand, generic, prod, implant, combo], dim=1)\n",
        "            dev_id_emb = self.dev_proj(dev_cat)  # [N_dev, emb]\n",
        "\n",
        "            if \"device_text\" in x_dict:\n",
        "                dev_txt = x_dict[\"device_text\"]        # [N_dev, 384], float\n",
        "                dev_txt_emb = self.device_text_proj(dev_txt)\n",
        "                out[\"device\"] = dev_id_emb + dev_txt_emb\n",
        "            else:\n",
        "                out[\"device\"] = dev_id_emb\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def prune_batch_dicts(x_dict, edge_index_dict):\n",
        "    keep_nodes = {nt for nt, x in x_dict.items() if x.size(0) > 0}\n",
        "    x_pruned = {nt: x for nt, x in x_dict.items() if nt in keep_nodes}\n",
        "    ei_pruned = {}\n",
        "    for et, ei in edge_index_dict.items():\n",
        "        if ei.numel()==0:\n",
        "            continue\n",
        "        src, _, dst = et\n",
        "        if src in keep_nodes and dst in keep_nodes:\n",
        "            ei_pruned[et] = ei\n",
        "    return x_pruned, ei_pruned\n",
        "\n",
        "\n",
        "sizes = (len(manuf), len(brand2ix), len(generic2ix),\n",
        "         len(prod2ix), len(report), len(event))\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "auroc = BinaryAUROC().to(device_cuda)\n",
        "f1 = BinaryF1Score().to(device_cuda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EGFIFPhJ84j"
      },
      "source": [
        "### 9b) Model Class Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJQMrrk_Gjj_"
      },
      "source": [
        "###HAN-LP (relation‑level attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Cd0Aw1LeGmVf"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import HANConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HAN_LP(nn.Module):\n",
        "    def __init__(self, metadata, sizes, emb=EMB, heads=4):\n",
        "        super().__init__()\n",
        "        (num_manuf, brand_sz, gen_sz, prod_sz, num_report, num_event) = sizes\n",
        "\n",
        "        self.fe = FeatureEmbedder(num_manuf, brand_sz, gen_sz, prod_sz,\n",
        "                                  num_report, num_event, emb=emb)\n",
        "\n",
        "        self.han = HANConv(\n",
        "            in_channels=emb,\n",
        "            out_channels=emb,\n",
        "            metadata=metadata,\n",
        "            heads=heads,\n",
        "        )\n",
        "\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x = self.fe(x_dict)\n",
        "        x, edge_index_dict = prune_batch_dicts(x, edge_index_dict)\n",
        "        if len(edge_index_dict) == 0:\n",
        "            return x\n",
        "        x = {k: F.relu(v).to(torch.float32) for k, v in x.items()}\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            x = self.han(x_dict=x, edge_index_dict=edge_index_dict)\n",
        "            x = {k: F.relu(v) for k, v in x.items()}\n",
        "        return x\n",
        "\n",
        "    def predict(self, x, edge_index, rel):\n",
        "        src, dst = rel[0], rel[2]\n",
        "        if (src not in x) or (dst not in x) or (edge_index.numel() == 0):\n",
        "            return torch.empty(0, device=list(x.values())[0].device if len(x) else \"cpu\")\n",
        "        h_src = x[src][edge_index[0]]\n",
        "        h_dst = x[dst][edge_index[1]]\n",
        "        return self.scale * (h_src * h_dst).sum(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFUd9iz2G34W"
      },
      "source": [
        "###Simple-HAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "8R2eAs8FG6vY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SimpleHAN_LP(nn.Module):\n",
        "    \"\"\"\n",
        "    HAN-style hetero encoder:\n",
        "      - One shared linear per edge type to map neighbor features\n",
        "      - Aggregate neighbor messages per relation\n",
        "      - Attention over relation-level messages for each node type\n",
        "      - Dot-product link predictor, same API as HGT_LP\n",
        "    \"\"\"\n",
        "    def __init__(self, metadata, sizes, emb=EMB, heads=4):\n",
        "        super().__init__()\n",
        "        (num_manuf, brand_sz, gen_sz, prod_sz, num_report, num_event) = sizes\n",
        "\n",
        "        self.fe = FeatureEmbedder(num_manuf, brand_sz, gen_sz, prod_sz,\n",
        "                                  num_report, num_event, emb=emb)\n",
        "\n",
        "        node_types, edge_types = metadata\n",
        "        self.node_types = list(node_types)\n",
        "        self.edge_types = list(edge_types)\n",
        "        self.emb = emb\n",
        "        self.heads = heads\n",
        "        self.head_dim = emb // heads\n",
        "\n",
        "        # linear transforms per edge type (relation-specific message functions)\n",
        "        self.rel_lin = nn.ModuleDict()\n",
        "        for (src, rel, dst) in self.edge_types:\n",
        "            key = f\"{src}__{rel}__{dst}\"\n",
        "            self.rel_lin[key] = nn.Linear(emb, emb, bias=False)\n",
        "\n",
        "        # attention parameters: one query per (dst node type, relation)\n",
        "        self.att_dst = nn.ModuleDict()\n",
        "        for nt in self.node_types:\n",
        "            self.att_dst[nt] = nn.Linear(emb, emb, bias=False)\n",
        "\n",
        "        # relation embeddings for attention keys\n",
        "        self.rel_att = nn.ParameterDict()\n",
        "        for (src, rel, dst) in self.edge_types:\n",
        "            key = f\"{src}__{rel}__{dst}\"\n",
        "            self.rel_att[key] = nn.Parameter(torch.randn(emb))\n",
        "\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "      # initial embeddings\n",
        "      x = self.fe(x_dict)\n",
        "      x, edge_index_dict = prune_batch_dicts(x, edge_index_dict)\n",
        "      if len(edge_index_dict) == 0:\n",
        "          return x\n",
        "\n",
        "      x = {k: F.relu(v).to(torch.float32) for k, v in x.items()}\n",
        "\n",
        "      # for each destination node type, collect relation-level messages\n",
        "      rel_msgs_per_dst = {nt: [] for nt in self.node_types}\n",
        "      rel_keys_per_dst = {nt: [] for nt in self.node_types}\n",
        "\n",
        "      for (src_nt, rel_name, dst_nt) in self.edge_types:\n",
        "          et = (src_nt, rel_name, dst_nt)\n",
        "          if et not in edge_index_dict:\n",
        "              continue\n",
        "          if (src_nt not in x) or (dst_nt not in x):\n",
        "              continue\n",
        "\n",
        "          ei = edge_index_dict[et]  # [2, E]\n",
        "          if ei.numel() == 0:\n",
        "              continue\n",
        "\n",
        "          key = f\"{src_nt}__{rel_name}__{dst_nt}\"\n",
        "\n",
        "          h_src = x[src_nt]\n",
        "          h_dst = x[dst_nt]\n",
        "\n",
        "          # linear transform on source features\n",
        "          m_src = self.rel_lin[key](h_src)\n",
        "          m_src = m_src.to(torch.float32)\n",
        "\n",
        "          src_idx = ei[0]\n",
        "          dst_idx = ei[1]\n",
        "\n",
        "          # aggregate messages to dst nodes\n",
        "          m = torch.zeros_like(h_dst)\n",
        "          m.index_add_(0, dst_idx, m_src[src_idx])\n",
        "\n",
        "          deg = torch.zeros(h_dst.size(0), device=h_dst.device, dtype=torch.float32)\n",
        "          deg.index_add_(0, dst_idx, torch.ones_like(dst_idx, dtype=torch.float32))\n",
        "          deg = deg.clamp(min=1.0).unsqueeze(-1)\n",
        "          m = m / deg  # [N_dst, emb]\n",
        "\n",
        "          rel_msgs_per_dst[dst_nt].append(m)\n",
        "          rel_keys_per_dst[dst_nt].append(key)\n",
        "\n",
        "      # for each dst node type, apply attention over relation messages\n",
        "      out = {}\n",
        "      for dst_nt, msgs in rel_msgs_per_dst.items():\n",
        "          if dst_nt not in x:\n",
        "              continue\n",
        "          h_dst = x[dst_nt]  # [N, emb], float32\n",
        "\n",
        "          if len(msgs) == 0:\n",
        "              # keep original embedding\n",
        "              out[dst_nt] = h_dst\n",
        "              continue\n",
        "\n",
        "          # stack relation messages: [R, N, emb]\n",
        "          M = torch.stack(msgs, dim=0)\n",
        "          R, N, D = M.shape\n",
        "\n",
        "          q = self.att_dst[dst_nt](h_dst)  # [N, emb]\n",
        "\n",
        "          # relation keys: [R, emb]\n",
        "          rel_keys = torch.stack([self.rel_att[k] for k in rel_keys_per_dst[dst_nt]], dim=0)\n",
        "\n",
        "          # attention logits: [R, N]\n",
        "          att_logits = torch.matmul(rel_keys, q.t()) / (D ** 0.5)\n",
        "          att_weights = F.softmax(att_logits, dim=0)\n",
        "\n",
        "          # weighted sum of messages: [N, emb]\n",
        "          att_expanded = att_weights.unsqueeze(-1)  # [R, N, 1]\n",
        "          h_out = (att_expanded * M).sum(dim=0)     # [N, emb]\n",
        "          out[dst_nt] = F.relu(h_out)\n",
        "\n",
        "      return out\n",
        "\n",
        "\n",
        "    def predict(self, x, edge_index, rel):\n",
        "        src, dst = rel[0], rel[2]\n",
        "        if (src not in x) or (dst not in x) or (edge_index.numel() == 0):\n",
        "            return torch.empty(0, device=list(x.values())[0].device if len(x) else \"cpu\")\n",
        "        h_src = x[src][edge_index[0]]\n",
        "        h_dst = x[dst][edge_index[1]]\n",
        "        return self.scale * (h_src * h_dst).sum(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKbjCl7KZgol"
      },
      "source": [
        "### HGT-CONV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "F_HpoSPxZcQB"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import HGTConv\n",
        "\n",
        "HGT_HEADS = 1\n",
        "\n",
        "class HGT_LP(nn.Module):\n",
        "    def __init__(self, metadata, sizes, heads=HGT_HEADS, emb=EMB):\n",
        "        super().__init__()\n",
        "        (num_manuf, brand_sz, gen_sz, prod_sz, num_report, num_event) = sizes\n",
        "        self.fe = FeatureEmbedder(num_manuf, brand_sz, gen_sz, prod_sz, num_report, num_event, emb=emb)\n",
        "        self.hgt = HGTConv(in_channels=emb, out_channels=emb, metadata=metadata, heads=heads)\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x = self.fe(x_dict)\n",
        "        x, edge_index_dict = prune_batch_dicts(x, edge_index_dict)\n",
        "        if len(edge_index_dict) == 0:\n",
        "            return x\n",
        "        x = {k: F.relu(v).to(torch.float32) for k, v in x.items()}\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            x = self.hgt(x, edge_index_dict)\n",
        "            x = {k: F.relu(v) for k, v in x.items()}\n",
        "        return x\n",
        "\n",
        "    def predict(self, x, edge_index, rel):\n",
        "        src, dst = rel[0], rel[2]\n",
        "        if (src not in x) or (dst not in x) or (edge_index.numel()==0):\n",
        "            return torch.empty(0, device=list(x.values())[0].device if len(x) else \"cpu\")\n",
        "        h_src = x[src][edge_index[0]]\n",
        "        h_dst = x[dst][edge_index[1]]\n",
        "        return self.scale * (h_src * h_dst).sum(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_yh_VP4Z0qt"
      },
      "source": [
        "###R-GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "osc4g1JyZ5Od"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import RGCNConv\n",
        "\n",
        "class RGCN_LP(nn.Module):\n",
        "    def __init__(self, metadata, sizes, emb=EMB, hidden=None):\n",
        "        super().__init__()\n",
        "        if hidden is None:\n",
        "            hidden = emb\n",
        "\n",
        "        (num_manuf, brand_sz, gen_sz, prod_sz, num_report, num_event) = sizes\n",
        "        self.fe = FeatureEmbedder(num_manuf, brand_sz, gen_sz, prod_sz,\n",
        "                                  num_report, num_event, emb=emb)\n",
        "\n",
        "        node_types, edge_types = metadata\n",
        "        self.node_types = list(node_types)\n",
        "        self.edge_types = list(edge_types)\n",
        "\n",
        "        # map each node type to a global id range\n",
        "        self.nt_offsets = {}\n",
        "        offset = 0\n",
        "        for nt in self.node_types:\n",
        "            self.nt_offsets[nt] = offset\n",
        "            offset += 1_000_000_000\n",
        "\n",
        "        # relation id for each edge type\n",
        "        self.rel2id = {et: i for i, et in enumerate(self.edge_types)}\n",
        "        num_rel = len(self.edge_types)\n",
        "\n",
        "        self.conv1 = RGCNConv(emb, hidden, num_relations=num_rel)\n",
        "        self.conv2 = RGCNConv(hidden, emb, num_relations=num_rel)\n",
        "\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def _build_homogeneous(self, x_dict, edge_index_dict):\n",
        "        # x_dict: {nt: [N_nt, emb]}\n",
        "        nt_slices = {}\n",
        "        cur = 0\n",
        "        for nt, x in x_dict.items():\n",
        "            N = x.size(0)\n",
        "            nt_slices[nt] = (cur, cur + N)\n",
        "            cur += N\n",
        "\n",
        "        # concatenate node features\n",
        "        x_cat = torch.cat([x_dict[nt] for nt in self.node_types if nt in x_dict], dim=0)\n",
        "\n",
        "        # build homogeneous edge_index and edge_type tensors\n",
        "        edge_index_list = []\n",
        "        edge_type_list = []\n",
        "        for et in self.edge_types:\n",
        "            if et not in edge_index_dict:\n",
        "                continue\n",
        "            src_nt, _, dst_nt = et\n",
        "            if src_nt not in nt_slices or dst_nt not in nt_slices:\n",
        "                continue\n",
        "            ei = edge_index_dict[et]\n",
        "            src_off, _ = nt_slices[src_nt]\n",
        "            dst_off, _ = nt_slices[dst_nt]\n",
        "            edge_index_list.append(torch.stack([\n",
        "                ei[0] + src_off,\n",
        "                ei[1] + dst_off,\n",
        "            ], dim=0))\n",
        "            edge_type_list.append(\n",
        "                torch.full((ei.size(1),), self.rel2id[et],\n",
        "                           dtype=torch.long, device=ei.device)\n",
        "            )\n",
        "\n",
        "        if len(edge_index_list) == 0:\n",
        "            return x_cat, None, None, nt_slices\n",
        "\n",
        "        edge_index = torch.cat(edge_index_list, dim=1)\n",
        "        edge_type  = torch.cat(edge_type_list, dim=0)\n",
        "        return x_cat, edge_index, edge_type, nt_slices\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        # embed hetero features\n",
        "        x = self.fe(x_dict)\n",
        "        x, edge_index_dict = prune_batch_dicts(x, edge_index_dict)\n",
        "        if len(edge_index_dict) == 0:\n",
        "            return x\n",
        "        x = {k: F.relu(v).to(torch.float32) for k, v in x.items()}\n",
        "\n",
        "        x_cat, edge_index, edge_type, nt_slices = self._build_homogeneous(x, edge_index_dict)\n",
        "        if edge_index is None:\n",
        "            return x\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            h = self.conv1(x_cat, edge_index, edge_type)\n",
        "            h = F.relu(h)\n",
        "            h = self.conv2(h, edge_index, edge_type)\n",
        "            h = F.relu(h)\n",
        "\n",
        "        # split back to hetero dict\n",
        "        out = {}\n",
        "        for nt, (s, e) in nt_slices.items():\n",
        "            out[nt] = h[s:e]\n",
        "        return out\n",
        "\n",
        "    def predict(self, x, edge_index, rel):\n",
        "        src, dst = rel[0], rel[2]\n",
        "        if (src not in x) or (dst not in x) or (edge_index.numel() == 0):\n",
        "            return torch.empty(0, device=list(x.values())[0].device if len(x) else \"cpu\")\n",
        "        h_src = x[src][edge_index[0]]\n",
        "        h_dst = x[dst][edge_index[1]]\n",
        "        return self.scale * (h_src * h_dst).sum(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "AyHzs4W9Zq7v"
      },
      "outputs": [],
      "source": [
        "def to_device_batch(batch, device):\n",
        "    # move base node features and edges\n",
        "    x  = {k: v.to(device, non_blocking=True) for k, v in batch.x_dict.items()}\n",
        "    ei = {k: v.to(device, non_blocking=True) for k, v in batch.edge_index_dict.items()}\n",
        "\n",
        "    # Attach text embeddings using the local node id -> global id mapping from the batch\n",
        "    # For LinkNeighborLoader on HeteroData, each node type has a `batch[nt].n_id`\n",
        "    if hasattr(batch, \"n_id\"):\n",
        "        batch.n_id = batch.n_id.to(device, non_blocking=True)\n",
        "    if hasattr(batch, \"input_id\"):\n",
        "        batch.input_id = batch.input_id.to(device, non_blocking=True)\n",
        "\n",
        "    # Device text\n",
        "    if \"device\" in batch:\n",
        "        dev_global_ids = batch[\"device\"].n_id.cpu().numpy()\n",
        "        x[\"device_text\"] = device_text_emb[dev_global_ids].to(device, non_blocking=True)\n",
        "\n",
        "    # Report text\n",
        "    if \"report\" in batch:\n",
        "        rep_global_ids = batch[\"report\"].n_id.cpu().numpy()\n",
        "        x[\"report_text\"] = report_text_emb[rep_global_ids].to(device, non_blocking=True)\n",
        "\n",
        "    return x, ei\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Models (HAN_LP, Simple HAN, HGT, RGCN"
      ],
      "metadata": {
        "id": "6m_0n6HW9ACM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global training hyperparameters\n",
        "LR = 1e-3\n",
        "WD = 1e-4\n",
        "use_amp_globally = torch.cuda.is_available()\n",
        "\n",
        "def make_optimizer(model, lr=LR, wd=WD):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scaler = torch.cuda.amp.GradScaler(\n",
        "        enabled=use_amp_globally and not isinstance(model, SimpleHAN_LP)\n",
        "    )\n",
        "    return opt, scaler\n"
      ],
      "metadata": {
        "id": "lKb_FiciKvwT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def best_f1_threshold(probs, labels, num_steps=19):\n",
        "    \"\"\"\n",
        "    probs, labels: 1D torch tensors on CPU\n",
        "    Returns (best_threshold, best_f1)\n",
        "    \"\"\"\n",
        "    thresholds = torch.linspace(0.05, 0.95, steps=num_steps)\n",
        "    best_t, best_f1 = 0.5, 0.0\n",
        "    for t in thresholds:\n",
        "        preds = (probs >= t).int()\n",
        "        f1_t = f1_score(labels.numpy(), preds.numpy())\n",
        "        if f1_t > best_f1:\n",
        "            best_f1, best_t = f1_t, float(t)\n",
        "    return best_t, best_f1"
      ],
      "metadata": {
        "id": "9VDpadyhQZ77"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(name, model, max_epochs=50, patience=5):\n",
        "    model.to(device_cuda)\n",
        "\n",
        "    best = {\n",
        "        \"val_au\": -1.0,\n",
        "        \"test_au\": -1.0,\n",
        "        \"val_f1\": -1.0,\n",
        "        \"test_f1\": -1.0,\n",
        "        \"epoch\": -1,\n",
        "        \"state_dict\": None,\n",
        "    }\n",
        "\n",
        "    opt, scaler = make_optimizer(model, lr=LR, wd=WD)\n",
        "    use_amp = torch.cuda.is_available() and not isinstance(model, SimpleHAN_LP)\n",
        "\n",
        "    def run_epoch(loader, train_mode: bool):\n",
        "        if loader is None:\n",
        "            return {\"loss\": 0.0, \"au\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "        if train_mode:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        tot_loss = tot_au = tot_f1 = tot_n = 0\n",
        "\n",
        "        for i, batch in enumerate(loader, 1):\n",
        "            x, ei = to_device_batch(batch, device_cuda)\n",
        "            y     = batch[REL].edge_label.to(device_cuda).float()\n",
        "            e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "            if train_mode:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast(dtype=amp_dtype, enabled=True):\n",
        "                    h = model(x, ei)\n",
        "                    logits = model.predict(h, e_ind, REL)\n",
        "            else:\n",
        "                h = model(x, ei)\n",
        "                logits = model.predict(h, e_ind, REL)\n",
        "\n",
        "            if logits.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            y_use = y[:logits.numel()]\n",
        "            probs = torch.sigmoid(logits)\n",
        "            n = y_use.numel()\n",
        "\n",
        "            if train_mode:\n",
        "                loss = bce(logits, y_use)\n",
        "                if use_amp:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(opt)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "                tot_loss += float(loss.item()) * n\n",
        "            else:\n",
        "                loss = bce(logits, y_use)\n",
        "                tot_loss += float(loss.item()) * n\n",
        "\n",
        "            tot_au  += float(auroc(probs, y_use.int())) * n\n",
        "            tot_f1  += float(f1((probs > 0.5).int(), y_use.int())) * n\n",
        "            tot_n   += int(n)\n",
        "\n",
        "        if tot_n == 0:\n",
        "            return {\"loss\": 0.0, \"au\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "        return {\n",
        "            \"loss\": tot_loss / tot_n,\n",
        "            \"au\":   tot_au   / tot_n,\n",
        "            \"f1\":   tot_f1   / tot_n,\n",
        "        }\n",
        "\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        train_stats = run_epoch(train_loader, train_mode=True)\n",
        "        val_stats   = run_epoch(val_loader,   train_mode=False)\n",
        "        test_stats  = run_epoch(test_loader,  train_mode=False)\n",
        "\n",
        "        val_au  = val_stats[\"au\"]\n",
        "        val_f1  = val_stats[\"f1\"]\n",
        "        test_au = test_stats[\"au\"]\n",
        "        test_f1 = test_stats[\"f1\"]\n",
        "\n",
        "        print(f\"[{epoch:02d}] loss={train_stats['loss']:.4f} \"\n",
        "              f\"| train AU={train_stats['au']:.3f} \"\n",
        "              f\"| val AU={val_au:.3f} F1={val_f1:.3f} \"\n",
        "              f\"| test AU={test_au:.3f} F1={test_f1:.3f}\")\n",
        "\n",
        "        if val_au > best[\"val_au\"] + 1e-4:\n",
        "            best.update({\n",
        "                \"val_au\": float(val_au),\n",
        "                \"test_au\": float(test_au),\n",
        "                \"val_f1\": float(val_f1),\n",
        "                \"test_f1\": float(test_f1),\n",
        "                \"epoch\": epoch,\n",
        "                \"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "            })\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch} (best at {best['epoch']})\")\n",
        "                break\n",
        "\n",
        "    if best[\"state_dict\"] is not None:\n",
        "        model.load_state_dict(best[\"state_dict\"])\n",
        "\n",
        "    print(\n",
        "        f\"{name} best (by val AU): \"\n",
        "        f\"{{'val_au': {best['val_au']}, 'test_au': {best['test_au']}, \"\n",
        "        f\"'epoch': {best['epoch']}, 'val_f1': {best['val_f1']}, 'test_f1': {best['test_f1']}}}\"\n",
        "    )\n",
        "\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "YrOnADWQJxEG"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY34eu3sG6TO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560e9216-3d07-4a01-acb0-81f32a006ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of train loader: 373\n",
            "[01] loss=0.4963 | train AU=0.850 | val AU=0.212 F1=0.000 | test AU=0.217 F1=0.000\n",
            "[02] loss=0.4492 | train AU=0.889 | val AU=0.240 F1=0.000 | test AU=0.241 F1=0.000\n",
            "[03] loss=0.4306 | train AU=0.900 | val AU=0.259 F1=0.000 | test AU=0.261 F1=0.000\n",
            "[04] loss=0.4110 | train AU=0.914 | val AU=0.273 F1=0.000 | test AU=0.269 F1=0.000\n",
            "[05] loss=0.4051 | train AU=0.913 | val AU=0.275 F1=0.000 | test AU=0.278 F1=0.000\n",
            "[06] loss=0.3933 | train AU=0.922 | val AU=0.293 F1=0.000 | test AU=0.294 F1=0.000\n",
            "[07] loss=0.3844 | train AU=0.928 | val AU=0.317 F1=0.000 | test AU=0.318 F1=0.000\n",
            "[08] loss=0.3825 | train AU=0.925 | val AU=0.328 F1=0.000 | test AU=0.334 F1=0.000\n",
            "[09] loss=0.3746 | train AU=0.931 | val AU=0.339 F1=0.000 | test AU=0.344 F1=0.000\n",
            "[10] loss=0.3705 | train AU=0.931 | val AU=0.352 F1=0.000 | test AU=0.351 F1=0.000\n"
          ]
        }
      ],
      "source": [
        "print(f\"length of train loader: {len(train_loader)}\")\n",
        "\n",
        "model_han_simple = SimpleHAN_LP(data.metadata(), sizes, emb=EMB, heads=4).to(device_cuda)\n",
        "best_han_simple = train_model(\"SimpleHAN_LP\", model_han_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnE37Z06Gnpe"
      },
      "outputs": [],
      "source": [
        "print(f\"length of train loader: {len(train_loader)}\")\n",
        "model_han = HAN_LP(data.metadata(), sizes, emb=EMB, heads=4).to(device_cuda)\n",
        "best_han = train_model(\"HAN_LP\", model_han)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_q1z3drZzdX"
      },
      "outputs": [],
      "source": [
        "print(f\"length of train loader: {len(train_loader)}\")\n",
        "model_hgt = HGT_LP(data.metadata(), sizes, heads=HGT_HEADS, emb=EMB).to(device_cuda)\n",
        "best_hgt = train_model(\"HGT_LP\", model_hgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlZw0oC0Z2Vd"
      },
      "outputs": [],
      "source": [
        "print(f\"length of train loader: {len(train_loader)}\")\n",
        "model_rgcn = RGCN_LP(data.metadata(), sizes, emb=EMB).to(device_cuda)\n",
        "best_rgcn = train_model(\"RGCN_LP\", model_rgcn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SXJOVG8NOE7"
      },
      "source": [
        "### 9c) Save metrics + config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_91qj3LNgm1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "results = {\n",
        "    \"SimpleHAN_LP\": {\n",
        "        \"val_AUROC\": best_han_simple[\"val_au\"],\n",
        "        \"test_AUROC\": best_han_simple[\"test_au\"],\n",
        "        \"val_F1\": best_han_simple[\"val_f1\"],\n",
        "        \"test_F1\": best_han_simple[\"test_f1\"],\n",
        "        \"best_epoch\": best_han_simple[\"epoch\"],\n",
        "    },\n",
        "    \"HAN_LP\": {\n",
        "        \"val_AUROC\": best_han[\"val_au\"],\n",
        "        \"test_AUROC\": best_han[\"test_au\"],\n",
        "        \"val_F1\": best_han[\"val_f1\"],\n",
        "        \"test_F1\": best_han[\"test_f1\"],\n",
        "        \"best_epoch\": best_han[\"epoch\"],\n",
        "    },\n",
        "    \"HGT_LP\": {\n",
        "        \"val_AUROC\": best_hgt[\"val_au\"],\n",
        "        \"test_AUROC\": best_hgt[\"test_au\"],\n",
        "        \"val_F1\": best_hgt[\"val_f1\"],\n",
        "        \"test_F1\": best_hgt[\"test_f1\"],\n",
        "        \"best_epoch\": best_hgt[\"epoch\"],\n",
        "    },\n",
        "    \"RGCN_LP\": {\n",
        "        \"val_AUROC\": best_rgcn[\"val_au\"],\n",
        "        \"test_AUROC\": best_rgcn[\"test_au\"],\n",
        "        \"val_F1\": best_rgcn[\"val_f1\"],\n",
        "        \"test_F1\": best_rgcn[\"test_f1\"],\n",
        "        \"best_epoch\": best_rgcn[\"epoch\"],\n",
        "    },\n",
        "    \"config\": {\n",
        "        \"EMB\": EMB,\n",
        "        \"HGT_HEADS\": HGT_HEADS,\n",
        "        \"batch_train\": 512,\n",
        "        \"batch_eval\": 1024,\n",
        "        \"fanouts\": \"REL:[2], rev_REL:[2], others:[0]\",\n",
        "        \"neg_sampling_ratio\": 0.5,\n",
        "        \"models\": [\"SimpleHAN_LP\", \"HAN_LP\", \"HGT_LP\", \"RGCN_LP\"],\n",
        "        \"amp\": True,\n",
        "        \"split\": \"temporal within 2024 (train/val/test by DATE_RECEIVED)\",\n",
        "    },\n",
        "    \"notes\": \"Neighbor sampling focused on report->device; manufacturer/event relations present but sampled with 0-hop.\",\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfnNkxtinreP"
      },
      "source": [
        "### 10a) Plot/Overlay curves for all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8spCEsXunj3T"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import RocCurveDisplay, roc_curve, auc, precision_recall_curve\n",
        "\n",
        "models = {\n",
        "    \"HAN_LP\": model_han,\n",
        "    \"SimpleHAN_LP\": model_han_simple,\n",
        "    \"HGT\":  model_hgt,\n",
        "    \"RGCN\": model_rgcn\n",
        "}\n",
        "\n",
        "if (val_loader is None) or (len(val_loader) == 0):\n",
        "    print(\"No validation edges available for ROC/PR plotting.\")\n",
        "else:\n",
        "    curves = {}  # name -> dict with fpr,tpr,roc_auc,rec,prec,pr_auc\n",
        "\n",
        "    for name, mdl in models.items():\n",
        "        if mdl is None:\n",
        "            continue\n",
        "        mdl.eval()\n",
        "        probs_all, y_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader, 1):\n",
        "                x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "                ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "                y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "                e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "                h = mdl(x, ei)\n",
        "                logits = mdl.predict(h, e_ind, REL)\n",
        "                if logits.numel() == 0:\n",
        "                    continue\n",
        "                p = torch.sigmoid(logits)\n",
        "                n = min(20000, p.numel())\n",
        "                probs_all.append(p[:n].detach().cpu())\n",
        "                y_all.append(y[:n].detach().cpu())\n",
        "                if i >= 25:\n",
        "                    break\n",
        "\n",
        "        if len(probs_all) == 0:\n",
        "            print(f\"{name}: no non-empty validation batches for ROC/PR plotting.\")\n",
        "            continue\n",
        "\n",
        "        probs_all_np = torch.cat(probs_all).numpy()\n",
        "        y_all_np     = torch.cat(y_all).numpy()\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_all_np, probs_all_np)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        prec, rec, _ = precision_recall_curve(y_all_np, probs_all_np)\n",
        "        pr_auc = auc(rec, prec)\n",
        "\n",
        "        print(f\"{name}: Val ROC-AUC (subsample) = {roc_auc:.3f} | PR-AUC = {pr_auc:.3f}\")\n",
        "        curves[name] = {\n",
        "            \"fpr\": fpr, \"tpr\": tpr, \"roc_auc\": roc_auc,\n",
        "            \"rec\": rec, \"prec\": prec, \"pr_auc\": pr_auc,\n",
        "        }\n",
        "\n",
        "    if len(curves) == 0:\n",
        "        print(\"No curves to plot.\")\n",
        "    else:\n",
        "        # ROC plot\n",
        "        plt.figure(figsize=(5,5))\n",
        "        for name, c in curves.items():\n",
        "            plt.plot(c[\"fpr\"], c[\"tpr\"], label=f\"{name} (AUROC={c['roc_auc']:.3f})\")\n",
        "        plt.plot([0,1], [0,1], '--', color='gray')\n",
        "        plt.xlabel(\"FPR\")\n",
        "        plt.ylabel(\"TPR\")\n",
        "        plt.title(\"ROC (val, subsampled)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # PR plot\n",
        "        plt.figure(figsize=(5,5))\n",
        "        for name, c in curves.items():\n",
        "            plt.plot(c[\"rec\"], c[\"prec\"], label=f\"{name} (AUPR={c['pr_auc']:.3f})\")\n",
        "        plt.xlabel(\"Recall\")\n",
        "        plt.ylabel(\"Precision\")\n",
        "        plt.title(\"PR (val, subsampled)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6pz3i36T_9F"
      },
      "source": [
        "#### 10b) Qualitative: top-10 predicted report->device links on TEST split (with metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwReYAVwT4sl"
      },
      "outputs": [],
      "source": [
        "# Models to compare\n",
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,         # if available\n",
        "    \"HAN_simple\": model_han_simple,  # SimpleHAN_LP in main notebook\n",
        "}\n",
        "\n",
        "model_name = \"HGT\"  # change this to \"RGCN\", \"HAN\", or \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on TEST split (with metadata)\n",
        "device_meta = device_keys.set_index(\"device_id\")[[\n",
        "    \"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"BRAND_NAME\",\"GENERIC_NAME\",\"DEVICE_REPORT_PRODUCT_CODE\"\n",
        "]]\n",
        "manuf_meta  = manuf.set_index(\"manufacturer_id\")[[\"manufacturer_name\"]]\n",
        "report_meta = report.set_index(\"report_id\")[[\"report_key\"]]\n",
        "\n",
        "if (test_loader is None) or (len(test_loader) == 0):\n",
        "    print(\"No test edges available for qualitative inspection.\")\n",
        "else:\n",
        "    probs_all, y_all, e_all = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader, 1):\n",
        "            x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "            ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "            y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "            e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "            h = model(x, ei)\n",
        "            logits = model.predict(h, e_ind, REL)\n",
        "            if logits.numel() == 0:\n",
        "                continue\n",
        "            p = torch.sigmoid(logits)\n",
        "            probs_all.append(p.detach().cpu())\n",
        "            y_all.append(y.detach().cpu())\n",
        "            e_all.append(e_ind.detach().cpu())\n",
        "            if i >= 50:\n",
        "                break\n",
        "\n",
        "    if len(probs_all) == 0:\n",
        "        print(f\"{model_name}: no non-empty test batches for qualitative inspection.\")\n",
        "    else:\n",
        "        probs_all = torch.cat(probs_all)\n",
        "        y_all = torch.cat(y_all)\n",
        "        e_all = torch.cat(e_all, dim=1)\n",
        "\n",
        "        top_k = 10\n",
        "        top_idx = torch.argsort(-probs_all)[:top_k]\n",
        "\n",
        "        rep_ids = e_all[0, top_idx].numpy()\n",
        "        dev_ids = e_all[1, top_idx].numpy()\n",
        "        probs   = probs_all[top_idx].numpy()\n",
        "        labels  = y_all[top_idx].numpy()\n",
        "\n",
        "        rows = []\n",
        "        for rid, did, p, y_true in zip(rep_ids, dev_ids, probs, labels):\n",
        "            r_key = report_meta.loc[rid, \"report_key\"] if rid in report_meta.index else None\n",
        "            d_row = device_meta.loc[did] if did in device_meta.index else None\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"report_id\": int(rid),\n",
        "                \"report_key\": r_key,\n",
        "                \"device_id\": int(did),\n",
        "                \"manufacturer\": d_row[\"MANUFACTURER_D_NAME\"] if d_row is not None else None,\n",
        "                \"model_name\": d_row[\"MODEL_NUMBER\"] if d_row is not None else None,\n",
        "                \"brand\": d_row[\"BRAND_NAME\"] if d_row is not None else None,\n",
        "                \"generic\": d_row[\"GENERIC_NAME\"] if d_row is not None else None,\n",
        "                \"product_code\": d_row[\"DEVICE_REPORT_PRODUCT_CODE\"] if d_row is not None else None,\n",
        "                \"pred_prob\": float(p),\n",
        "                \"label\": float(y_true),\n",
        "            })\n",
        "\n",
        "        df_qual = pd.DataFrame(rows)\n",
        "        display(df_qual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to compare\n",
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,         # if available\n",
        "    \"HAN_simple\": model_han_simple,  # SimpleHAN_LP in main notebook\n",
        "}\n",
        "\n",
        "model_name = \"RGCN\"  # change this to \"RGCN\", \"HAN\", or \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on TEST split (with metadata)\n",
        "device_meta = device_keys.set_index(\"device_id\")[[\n",
        "    \"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"BRAND_NAME\",\"GENERIC_NAME\",\"DEVICE_REPORT_PRODUCT_CODE\"\n",
        "]]\n",
        "manuf_meta  = manuf.set_index(\"manufacturer_id\")[[\"manufacturer_name\"]]\n",
        "report_meta = report.set_index(\"report_id\")[[\"report_key\"]]\n",
        "\n",
        "if (test_loader is None) or (len(test_loader) == 0):\n",
        "    print(\"No test edges available for qualitative inspection.\")\n",
        "else:\n",
        "    probs_all, y_all, e_all = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader, 1):\n",
        "            x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "            ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "            y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "            e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "            h = model(x, ei)\n",
        "            logits = model.predict(h, e_ind, REL)\n",
        "            if logits.numel() == 0:\n",
        "                continue\n",
        "            p = torch.sigmoid(logits)\n",
        "            probs_all.append(p.detach().cpu())\n",
        "            y_all.append(y.detach().cpu())\n",
        "            e_all.append(e_ind.detach().cpu())\n",
        "            if i >= 50:\n",
        "                break\n",
        "\n",
        "    if len(probs_all) == 0:\n",
        "        print(f\"{model_name}: no non-empty test batches for qualitative inspection.\")\n",
        "    else:\n",
        "        probs_all = torch.cat(probs_all)\n",
        "        y_all = torch.cat(y_all)\n",
        "        e_all = torch.cat(e_all, dim=1)\n",
        "\n",
        "        top_k = 10\n",
        "        top_idx = torch.argsort(-probs_all)[:top_k]\n",
        "\n",
        "        rep_ids = e_all[0, top_idx].numpy()\n",
        "        dev_ids = e_all[1, top_idx].numpy()\n",
        "        probs   = probs_all[top_idx].numpy()\n",
        "        labels  = y_all[top_idx].numpy()\n",
        "\n",
        "        rows = []\n",
        "        for rid, did, p, y_true in zip(rep_ids, dev_ids, probs, labels):\n",
        "            r_key = report_meta.loc[rid, \"report_key\"] if rid in report_meta.index else None\n",
        "            d_row = device_meta.loc[did] if did in device_meta.index else None\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"report_id\": int(rid),\n",
        "                \"report_key\": r_key,\n",
        "                \"device_id\": int(did),\n",
        "                \"manufacturer\": d_row[\"MANUFACTURER_D_NAME\"] if d_row is not None else None,\n",
        "                \"model_name\": d_row[\"MODEL_NUMBER\"] if d_row is not None else None,\n",
        "                \"brand\": d_row[\"BRAND_NAME\"] if d_row is not None else None,\n",
        "                \"generic\": d_row[\"GENERIC_NAME\"] if d_row is not None else None,\n",
        "                \"product_code\": d_row[\"DEVICE_REPORT_PRODUCT_CODE\"] if d_row is not None else None,\n",
        "                \"pred_prob\": float(p),\n",
        "                \"label\": float(y_true),\n",
        "            })\n",
        "\n",
        "        df_qual = pd.DataFrame(rows)\n",
        "        display(df_qual)"
      ],
      "metadata": {
        "id": "N-eu01OdSaZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to compare\n",
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,         # if available\n",
        "    \"HAN_simple\": model_han_simple,  # SimpleHAN_LP in main notebook\n",
        "}\n",
        "\n",
        "model_name = \"HAN\"  # change this to \"RGCN\", \"HAN\", or \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on TEST split (with metadata)\n",
        "device_meta = device_keys.set_index(\"device_id\")[[\n",
        "    \"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"BRAND_NAME\",\"GENERIC_NAME\",\"DEVICE_REPORT_PRODUCT_CODE\"\n",
        "]]\n",
        "manuf_meta  = manuf.set_index(\"manufacturer_id\")[[\"manufacturer_name\"]]\n",
        "report_meta = report.set_index(\"report_id\")[[\"report_key\"]]\n",
        "\n",
        "if (test_loader is None) or (len(test_loader) == 0):\n",
        "    print(\"No test edges available for qualitative inspection.\")\n",
        "else:\n",
        "    probs_all, y_all, e_all = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader, 1):\n",
        "            x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "            ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "            y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "            e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "            h = model(x, ei)\n",
        "            logits = model.predict(h, e_ind, REL)\n",
        "            if logits.numel() == 0:\n",
        "                continue\n",
        "            p = torch.sigmoid(logits)\n",
        "            probs_all.append(p.detach().cpu())\n",
        "            y_all.append(y.detach().cpu())\n",
        "            e_all.append(e_ind.detach().cpu())\n",
        "            if i >= 50:\n",
        "                break\n",
        "\n",
        "    if len(probs_all) == 0:\n",
        "        print(f\"{model_name}: no non-empty test batches for qualitative inspection.\")\n",
        "    else:\n",
        "        probs_all = torch.cat(probs_all)\n",
        "        y_all = torch.cat(y_all)\n",
        "        e_all = torch.cat(e_all, dim=1)\n",
        "\n",
        "        top_k = 10\n",
        "        top_idx = torch.argsort(-probs_all)[:top_k]\n",
        "\n",
        "        rep_ids = e_all[0, top_idx].numpy()\n",
        "        dev_ids = e_all[1, top_idx].numpy()\n",
        "        probs   = probs_all[top_idx].numpy()\n",
        "        labels  = y_all[top_idx].numpy()\n",
        "\n",
        "        rows = []\n",
        "        for rid, did, p, y_true in zip(rep_ids, dev_ids, probs, labels):\n",
        "            r_key = report_meta.loc[rid, \"report_key\"] if rid in report_meta.index else None\n",
        "            d_row = device_meta.loc[did] if did in device_meta.index else None\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"report_id\": int(rid),\n",
        "                \"report_key\": r_key,\n",
        "                \"device_id\": int(did),\n",
        "                \"manufacturer\": d_row[\"MANUFACTURER_D_NAME\"] if d_row is not None else None,\n",
        "                \"model_name\": d_row[\"MODEL_NUMBER\"] if d_row is not None else None,\n",
        "                \"brand\": d_row[\"BRAND_NAME\"] if d_row is not None else None,\n",
        "                \"generic\": d_row[\"GENERIC_NAME\"] if d_row is not None else None,\n",
        "                \"product_code\": d_row[\"DEVICE_REPORT_PRODUCT_CODE\"] if d_row is not None else None,\n",
        "                \"pred_prob\": float(p),\n",
        "                \"label\": float(y_true),\n",
        "            })\n",
        "\n",
        "        df_qual = pd.DataFrame(rows)\n",
        "        display(df_qual)"
      ],
      "metadata": {
        "id": "Km-nQVBtShfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to compare\n",
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,         # if available\n",
        "    \"HAN_simple\": model_han_simple,  # SimpleHAN_LP in main notebook\n",
        "}\n",
        "\n",
        "model_name = \"HAN_simple\"  # change this to \"RGCN\", \"HAN\", or \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on TEST split (with metadata)\n",
        "device_meta = device_keys.set_index(\"device_id\")[[\n",
        "    \"MANUFACTURER_D_NAME\",\"MODEL_NUMBER\",\"BRAND_NAME\",\"GENERIC_NAME\",\"DEVICE_REPORT_PRODUCT_CODE\"\n",
        "]]\n",
        "manuf_meta  = manuf.set_index(\"manufacturer_id\")[[\"manufacturer_name\"]]\n",
        "report_meta = report.set_index(\"report_id\")[[\"report_key\"]]\n",
        "\n",
        "if (test_loader is None) or (len(test_loader) == 0):\n",
        "    print(\"No test edges available for qualitative inspection.\")\n",
        "else:\n",
        "    probs_all, y_all, e_all = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader, 1):\n",
        "            x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "            ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "            y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "            e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "            h = model(x, ei)\n",
        "            logits = model.predict(h, e_ind, REL)\n",
        "            if logits.numel() == 0:\n",
        "                continue\n",
        "            p = torch.sigmoid(logits)\n",
        "            probs_all.append(p.detach().cpu())\n",
        "            y_all.append(y.detach().cpu())\n",
        "            e_all.append(e_ind.detach().cpu())\n",
        "            if i >= 50:\n",
        "                break\n",
        "\n",
        "    if len(probs_all) == 0:\n",
        "        print(f\"{model_name}: no non-empty test batches for qualitative inspection.\")\n",
        "    else:\n",
        "        probs_all = torch.cat(probs_all)\n",
        "        y_all = torch.cat(y_all)\n",
        "        e_all = torch.cat(e_all, dim=1)\n",
        "\n",
        "        top_k = 10\n",
        "        top_idx = torch.argsort(-probs_all)[:top_k]\n",
        "\n",
        "        rep_ids = e_all[0, top_idx].numpy()\n",
        "        dev_ids = e_all[1, top_idx].numpy()\n",
        "        probs   = probs_all[top_idx].numpy()\n",
        "        labels  = y_all[top_idx].numpy()\n",
        "\n",
        "        rows = []\n",
        "        for rid, did, p, y_true in zip(rep_ids, dev_ids, probs, labels):\n",
        "            r_key = report_meta.loc[rid, \"report_key\"] if rid in report_meta.index else None\n",
        "            d_row = device_meta.loc[did] if did in device_meta.index else None\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"report_id\": int(rid),\n",
        "                \"report_key\": r_key,\n",
        "                \"device_id\": int(did),\n",
        "                \"manufacturer\": d_row[\"MANUFACTURER_D_NAME\"] if d_row is not None else None,\n",
        "                \"model_name\": d_row[\"MODEL_NUMBER\"] if d_row is not None else None,\n",
        "                \"brand\": d_row[\"BRAND_NAME\"] if d_row is not None else None,\n",
        "                \"generic\": d_row[\"GENERIC_NAME\"] if d_row is not None else None,\n",
        "                \"product_code\": d_row[\"DEVICE_REPORT_PRODUCT_CODE\"] if d_row is not None else None,\n",
        "                \"pred_prob\": float(p),\n",
        "                \"label\": float(y_true),\n",
        "            })\n",
        "\n",
        "        df_qual = pd.DataFrame(rows)\n",
        "        display(df_qual)"
      ],
      "metadata": {
        "id": "cEouQPyTSyYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FFebcb2Rqbd"
      },
      "source": [
        "#### 10c) Fetch top 10 predicted links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VneftPcOLVH"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,\n",
        "    \"HAN_simple\": model_han_simple,\n",
        "}\n",
        "\n",
        "model_name = \"HGT\"  # choose: \"HGT\", \"RGCN\", \"HAN\", \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on val split\n",
        "if (val_loader is None) or (len(val_loader) == 0):\n",
        "    print(\"No validation edges available for qualitative inspection.\")\n",
        "else:\n",
        "    batch = next(iter(val_loader))\n",
        "    x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "    ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "    y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "    e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h = model(x, ei)\n",
        "        logits = model.predict(h, e_ind, REL)\n",
        "        if logits.numel() == 0:\n",
        "            print(f\"{model_name}: validation batch contained no edges for REL.\")\n",
        "        else:\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            top = np.argsort(-probs)[:10]\n",
        "            pairs = list(\n",
        "                zip(\n",
        "                    e_ind[0, top].cpu().numpy(),\n",
        "                    e_ind[1, top].cpu().numpy(),\n",
        "                    probs[top],\n",
        "                    y[top].cpu().numpy(),\n",
        "                )\n",
        "            )\n",
        "            df_top = pd.DataFrame(pairs, columns=[\"report_id\",\"device_id\",\"pred_prob\",\"label\"])\n",
        "            df_top.insert(0, \"model\", model_name)\n",
        "            display(df_top)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,\n",
        "    \"HAN_simple\": model_han_simple,\n",
        "}\n",
        "\n",
        "model_name = \"RGCN\"  # choose: \"HGT\", \"RGCN\", \"HAN\", \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on val split\n",
        "if (val_loader is None) or (len(val_loader) == 0):\n",
        "    print(\"No validation edges available for qualitative inspection.\")\n",
        "else:\n",
        "    batch = next(iter(val_loader))\n",
        "    x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "    ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "    y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "    e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h = model(x, ei)\n",
        "        logits = model.predict(h, e_ind, REL)\n",
        "        if logits.numel() == 0:\n",
        "            print(f\"{model_name}: validation batch contained no edges for REL.\")\n",
        "        else:\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            top = np.argsort(-probs)[:10]\n",
        "            pairs = list(\n",
        "                zip(\n",
        "                    e_ind[0, top].cpu().numpy(),\n",
        "                    e_ind[1, top].cpu().numpy(),\n",
        "                    probs[top],\n",
        "                    y[top].cpu().numpy(),\n",
        "                )\n",
        "            )\n",
        "            df_top = pd.DataFrame(pairs, columns=[\"report_id\",\"device_id\",\"pred_prob\",\"label\"])\n",
        "            df_top.insert(0, \"model\", model_name)\n",
        "            display(df_top)\n"
      ],
      "metadata": {
        "id": "s3Y_lEdBS7ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,\n",
        "    \"HAN_simple\": model_han_simple,\n",
        "}\n",
        "\n",
        "model_name = \"HAN\"  # choose: \"HGT\", \"RGCN\", \"HAN\", \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on val split\n",
        "if (val_loader is None) or (len(val_loader) == 0):\n",
        "    print(\"No validation edges available for qualitative inspection.\")\n",
        "else:\n",
        "    batch = next(iter(val_loader))\n",
        "    x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "    ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "    y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "    e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h = model(x, ei)\n",
        "        logits = model.predict(h, e_ind, REL)\n",
        "        if logits.numel() == 0:\n",
        "            print(f\"{model_name}: validation batch contained no edges for REL.\")\n",
        "        else:\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            top = np.argsort(-probs)[:10]\n",
        "            pairs = list(\n",
        "                zip(\n",
        "                    e_ind[0, top].cpu().numpy(),\n",
        "                    e_ind[1, top].cpu().numpy(),\n",
        "                    probs[top],\n",
        "                    y[top].cpu().numpy(),\n",
        "                )\n",
        "            )\n",
        "            df_top = pd.DataFrame(pairs, columns=[\"report_id\",\"device_id\",\"pred_prob\",\"label\"])\n",
        "            df_top.insert(0, \"model\", model_name)\n",
        "            display(df_top)\n"
      ],
      "metadata": {
        "id": "3bWg6NXATAIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"HGT\":        model_hgt,\n",
        "    \"RGCN\":       model_rgcn,\n",
        "    \"HAN\":        model_han,\n",
        "    \"HAN_simple\": model_han_simple,\n",
        "}\n",
        "\n",
        "model_name = \"HAN_simple\"  # choose: \"HGT\", \"RGCN\", \"HAN\", \"HAN_simple\"\n",
        "model = models[model_name]\n",
        "\n",
        "# Qualitative: top-10 predicted report->device links on val split\n",
        "if (val_loader is None) or (len(val_loader) == 0):\n",
        "    print(\"No validation edges available for qualitative inspection.\")\n",
        "else:\n",
        "    batch = next(iter(val_loader))\n",
        "    x  = {k: v.to(device_cuda) for k,v in batch.x_dict.items()}\n",
        "    ei = {k: v.to(device_cuda) for k,v in batch.edge_index_dict.items()}\n",
        "    y  = batch[REL].edge_label.to(device_cuda).float()\n",
        "    e_ind = batch[REL].edge_label_index.to(device_cuda)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h = model(x, ei)\n",
        "        logits = model.predict(h, e_ind, REL)\n",
        "        if logits.numel() == 0:\n",
        "            print(f\"{model_name}: validation batch contained no edges for REL.\")\n",
        "        else:\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            top = np.argsort(-probs)[:10]\n",
        "            pairs = list(\n",
        "                zip(\n",
        "                    e_ind[0, top].cpu().numpy(),\n",
        "                    e_ind[1, top].cpu().numpy(),\n",
        "                    probs[top],\n",
        "                    y[top].cpu().numpy(),\n",
        "                )\n",
        "            )\n",
        "            df_top = pd.DataFrame(pairs, columns=[\"report_id\",\"device_id\",\"pred_prob\",\"label\"])\n",
        "            df_top.insert(0, \"model\", model_name)\n",
        "            display(df_top)\n"
      ],
      "metadata": {
        "id": "PsdWpruLTFoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-znoHSg1OZ8g"
      },
      "source": [
        "###10d) Relation importance perturbation test: How much does the model rely on manufacturer -> device edges?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQNEtlejfjVV"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_with_mfr_drop(model, drop_frac=0.5):\n",
        "    from copy import deepcopy\n",
        "\n",
        "    if test_loader is None:\n",
        "        return {\"au\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "    # shallow copy - avoid messing up training graph\n",
        "    data_pert = deepcopy(data)\n",
        "\n",
        "    et = (\"manufacturer\", \"makes\", \"device\")\n",
        "    if et not in data_pert.edge_types:\n",
        "        print(\"No manufacturer->device relation present.\")\n",
        "        return evaluate(model, test_loader)\n",
        "\n",
        "    ei = data_pert[et].edge_index\n",
        "    E = ei.size(1)\n",
        "    k_keep = int(E * (1.0 - drop_frac))\n",
        "    keep_idx = torch.randperm(E)[:k_keep]\n",
        "    data_pert[et].edge_index = ei[:, keep_idx]\n",
        "\n",
        "    # Also drop reverse direction if present\n",
        "    rev_et = (\"device\", \"made_by\", \"manufacturer\")\n",
        "    if rev_et in data_pert.edge_types:\n",
        "        ei_rev = data_pert[rev_et].edge_index\n",
        "        E_rev = ei_rev.size(1)\n",
        "        keep_idx_rev = torch.randperm(E_rev)[:k_keep]\n",
        "        data_pert[rev_et].edge_index = ei_rev[:, keep_idx_rev]\n",
        "\n",
        "    # Build a temporary test loader on perturbed graph, same edges/labels\n",
        "    pert_test_loader = LinkNeighborLoader(\n",
        "        data=data_pert,\n",
        "        num_neighbors=fanouts,\n",
        "        edge_label_index=(REL, temporal_edges[\"test\"]),\n",
        "        edge_label=torch.ones(temporal_edges[\"test\"].size(1), dtype=torch.float),\n",
        "        neg_sampling_ratio=0.5,\n",
        "        batch_size=1024,\n",
        "        shuffle=False,\n",
        "        replace=False,\n",
        "        pin_memory=False,\n",
        "        num_workers=0,\n",
        "        persistent_workers=False,\n",
        "    )\n",
        "\n",
        "    return evaluate(model, pert_test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_frac = 0.5  # perturbation strength\n",
        "\n",
        "for name, mdl in models.items():\n",
        "    if mdl is None:\n",
        "        continue\n",
        "\n",
        "    # Baseline on the original test_loader\n",
        "    base = evaluate(mdl, test_loader)\n",
        "\n",
        "    # Perturbed evaluation with manufacturer->device edges dropped\n",
        "    pert = evaluate_with_mfr_drop(mdl, drop_frac=drop_frac)\n",
        "\n",
        "    print(f\"[{name}] Base test  : AU={base['au']:.3f}, F1={base['f1']:.3f}\")\n",
        "    print(f\"[{name}] After drop: AU={pert['au']:.3f}, F1={pert['f1']:.3f}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "2Vteg_bkW6Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Local neighborhood visualization"
      ],
      "metadata": {
        "id": "b_Z3VHmxPFI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_report_neighborhood(report_id, max_neighbors=20):\n",
        "    # Build a small bipartite graph: report, its devices, and associated manufacturers\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add the report node\n",
        "    G.add_node(f\"R{report_id}\", type=\"report\")\n",
        "\n",
        "    # find devices linked to this report\n",
        "    mask = (edge_train[0] == report_id)  # or edge_test if you prefer test\n",
        "    dev_ids = edge_train[1, mask].cpu().numpy()\n",
        "    dev_ids = dev_ids[:max_neighbors]\n",
        "\n",
        "    for d in dev_ids:\n",
        "        G.add_node(f\"D{d}\", type=\"device\")\n",
        "        G.add_edge(f\"R{report_id}\", f\"D{d}\", rel=\"mentions\")\n",
        "\n",
        "        # manufacturers for this device\n",
        "        m_mask = (edge_manuf_device[1] == d)\n",
        "        m_ids = edge_manuf_device[0, m_mask].cpu().numpy()\n",
        "        for m in m_ids[:2]:  # limit for clarity\n",
        "            G.add_node(f\"M{m}\", type=\"manufacturer\")\n",
        "            G.add_edge(f\"M{m}\", f\"D{d}\", rel=\"makes\")\n",
        "\n",
        "    # Layout and draw\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    colors = []\n",
        "    for n, data_n in G.nodes(data=True):\n",
        "        if data_n[\"type\"] == \"report\":\n",
        "            colors.append(\"tab:red\")\n",
        "        elif data_n[\"type\"] == \"device\":\n",
        "            colors.append(\"tab:blue\")\n",
        "        else:\n",
        "            colors.append(\"tab:green\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    nx.draw(G, pos, with_labels=True, node_color=colors, node_size=500, arrows=False, font_size=8)\n",
        "    edge_labels = nx.get_edge_attributes(G, \"rel\")\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
        "    plt.title(f\"Local neighborhood for report_id={report_id}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ny0_CXTTPG7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_report_neighborhood(report_id=0)"
      ],
      "metadata": {
        "id": "g0NQPWVgPKP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Convert ipynb to html (optional)"
      ],
      "metadata": {
        "id": "vDz4g7hhYooC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEJz53CkUl3G"
      },
      "outputs": [],
      "source": [
        "!jupyter nbconvert --to html /content/project.ipynb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}